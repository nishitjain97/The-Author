{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TheAuthor.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fREXEhW7FXH"
      },
      "source": [
        "# The Author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-16i55Zk7FXN"
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGVwNpTZ7FXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24ac4f9-c74c-47c4-a18d-9927f77edb13"
      },
      "source": [
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified %s' % filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "    return filename\n",
        "\n",
        "filename = maybe_download('./drive/MyDrive/Dataset/text8.zip', 31344016)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified ./drive/MyDrive/Dataset/text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5an1aoD97FXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb025c8-6cc2-43ba-fa69-04b8e6ede156"
      },
      "source": [
        "def clean_character(char):\n",
        "    if char in string.ascii_lowercase:\n",
        "        return char\n",
        "    elif char in string.ascii_uppercase:\n",
        "        return char.lower()\n",
        "    elif char == ' ':\n",
        "        return char\n",
        "    return ''\n",
        "\n",
        "def read_data(filename):\n",
        "    if os.path.splitext(filename)[1] == '.zip':\n",
        "        with zipfile.ZipFile(filename) as f:\n",
        "            name = f.namelist()[0]\n",
        "            data = tf.compat.as_str(f.read(name))\n",
        "    elif os.path.splitext(filename)[1] == '.txt':\n",
        "        with open(filename, 'r') as f:\n",
        "            data = f.readlines()\n",
        "            data = ''.join(data)\n",
        "    return data\n",
        "  \n",
        "text8 = read_data(filename)\n",
        "text8 = ''.join([clean_character(char) for char in text8])\n",
        "\n",
        "hp_path = \"./drive/MyDrive/Dataset/HarryPotter/\"\n",
        "harry_potter = ''.join([read_data(filename) \n",
        "                        for filename in [os.path.join(hp_path, path) \n",
        "                                         for path in os.listdir(hp_path)]])\n",
        "harry_potter = ''.join([clean_character(char) for char in harry_potter])\n",
        "print('Data size Text8 %d' % len(text8))\n",
        "print('Data size Harry Potter %d' % len(harry_potter))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size Text8 100000000\n",
            "Data size Harry Potter 5893451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGM3M2nN7FXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7c8e26-0865-4120-d82c-97262a286a91"
      },
      "source": [
        "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
        "first_letter = ord(string.ascii_lowercase[0])\n",
        "\n",
        "def char2id(char):\n",
        "    if char in string.ascii_lowercase:\n",
        "        return ord(char) - first_letter + 1\n",
        "    elif char == ' ':\n",
        "        return 0\n",
        "    else:\n",
        "        print('Unexpected character: %s' % char)\n",
        "        return 0\n",
        "    \n",
        "def id2char(dictid):\n",
        "    if dictid > 0:\n",
        "        return chr(dictid + first_letter - 1)\n",
        "    else:\n",
        "        return ' '\n",
        "\n",
        "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
        "print(id2char(1), id2char(26), id2char(0))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unexpected character: ï\n",
            "1 26 0 0\n",
            "a z  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be9gWY647FXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4924bdc-7990-42c0-f736-4555c59b0ac7"
      },
      "source": [
        "valid_size = 1000\n",
        "train_text = text8\n",
        "tune_text = harry_potter[:valid_size]\n",
        "valid_text = harry_potter[valid_size:]\n",
        "train_size = len(train_text)\n",
        "tune_size = len(tune_text)\n",
        "print(train_size + tune_size, train_text[:64])\n",
        "print(valid_size, valid_text[:64])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100001000  anarchism originated as a term of abuse first used against earl\n",
            "1000 nd the sorcerers stone  by jk rowlingp cmsummary rescued from th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBba07vu7FXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a347606-700b-45bb-a259-82355c5e118b"
      },
      "source": [
        "batch_size = 64\n",
        "num_unrollings = 10\n",
        "\n",
        "class BatchGenerator(object):\n",
        "    def __init__(self, text, batch_size, num_unrollings):\n",
        "        self._text = text\n",
        "        self._text_size = len(text)\n",
        "        self._batch_size = batch_size\n",
        "        self._num_unrollings = num_unrollings\n",
        "        segment = self._text_size // batch_size\n",
        "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
        "        self._last_batch = self._next_batch()\n",
        "        \n",
        "    def _next_batch(self):\n",
        "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
        "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
        "        for b in range(self._batch_size):\n",
        "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
        "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
        "        return batch\n",
        "    \n",
        "    def next(self):\n",
        "        \"\"\"\n",
        "            Generate the next array of batches from the data. The array consists of\n",
        "            the last batch of the previous array, followed by num_unrollings new ones.\n",
        "        \"\"\"\n",
        "        batches = [self._last_batch]\n",
        "        for step in range(self._num_unrollings):\n",
        "            batches.append(self._next_batch())\n",
        "        self._last_batch = batches[-1]\n",
        "        return batches\n",
        "    \n",
        "def characters(probabilities):\n",
        "    \"\"\"\n",
        "        Turn a 1-hot encoding or a probability distribution over the possible characters\n",
        "        back into its (most likely) character representation.\n",
        "    \"\"\"\n",
        "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
        "\n",
        "def batches2string(batches):\n",
        "    \"\"\"\n",
        "        Convert a sequence of batches back into their (most likely) string representation\n",
        "    \"\"\"\n",
        "    s = [''] * batches[0].shape[0]\n",
        "    for b in batches:\n",
        "        s = [''.join(x) for x in zip(s, characters(b))]\n",
        "    return s\n",
        "\n",
        "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
        "tune_batches = BatchGenerator(tune_text, batch_size, num_unrollings)\n",
        "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
        "\n",
        "print(batches2string(train_batches.next()))\n",
        "print(batches2string(train_batches.next()))\n",
        "print(batches2string(tune_batches.next()))\n",
        "print(batches2string(tune_batches.next()))\n",
        "print(batches2string(valid_batches.next()))\n",
        "print(batches2string(valid_batches.next()))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' anarchism ', 'ion from eu', 'esident lyn', 'r professio', ' minting af', 'concentrati', 'poken today', 'ber of hote', ' the bell a', 'overty with', 'anufacturer', ' britannica', 'd so david ', 'ea of worke', ' informatio', ' one hand a', 'hannel isla', 'ater reprin', 'r an invest', ' infinite a', 's explained', 'on of plant', ' be solved ', 'n nine resp', 'ation vital', 'self contai', 'ountless se', 'ce arag leg', 'are often c', 'ions eml th', 'ne such as ', 's livin thi', 'ecommunicat', ' one nine n', 'e six two z', ' computer g', 'ing to quan', 'erbicide to', ' special re', ' exotic per', 'rm indicati', 'a secular r', 'the frequen', ' synthesize', 'structural ', ' the media ', 'ail of tear', 'stems able ', 'anded by si', 'rophy assoc', ' union s pi', 'ired differ', 'hough not i', 'e minister ', 'y the king ', 'eudo histor', 'nounced at ', 'tics gullbe', 'sance compa', ' colleges w', 'et and sunr', 'e american ', 'he mass app', 'made such d']\n",
            "[' originated', 'urope aided', 'ndon johnso', 'on allowanc', 'fonso also ', 'ion camp wh', 'y as a firs', 'els and fin', 'aircraft fa', 'h the help ', 'rs such as ', 'a successio', ' collects t', 'ers self ma', 'on the meda', 'and removin', 'and french ', 'nted in boo', 'tigation of', 'amount of t', 'd above is ', 't shoots cr', ' and which ', 'pectively a', 'l for contr', 'ined system', 'exual perve', 'gal insuran', 'called in b', 'his is used', ' esoteric b', 'ing remade ', 'tions techn', 'nine two th', 'zero zero f', 'game afterb', 'ntum mechan', 'o the exten', 'elativity s', 'rfections t', 'ing that th', 'republic la', 'ncy r sqrt ', 'ed retrovir', ' reforms fa', ' announced ', 'rs the rise', ' to run on ', 'ir harry sm', 'ciation a c', 'iatiletka o', 'rent combin', 'itself guil', ' final year', ' serves a l', 'ry geoffrey', ' ricky s cl', 'erg jan mat', 'any and a m', 'which gradu', 'rise occur ', ' indian res', 'peal of dev', 'devices pos']\n",
            "['harry potte', 'd the sorce', ' stonebyj k', 'lingillustr', 'ns by mary ', 'dprarthur a', 'ine booksan', 'rint of sch', 'tic pressfo', 'ssica who l', ' storiesfor', 'e who loved', 'm tooand fo', ' who heard ', ' one firstt', 'copyright  ', 'jk rowlingi', 'trations by', 'y grandpr c', 'ight   warn', 'rosall righ', 'eserved pub', 'ed by schol', 'c press a d', 'ion of scho', 'ic incpubli', 's since sch', 'tic scholas', 'press and t', 'antern logo', 'trademarks ', 'r registere', 'ademarks of', 'olastic inc', 'y potter an', 'l related c', 'cters and e', 'nts are tra', 'rks of warn', 'rosno part ', 'his publica', ' may be rep', 'ced or stor', 'n a retriev', 'ystem or tr', 'itted in an', 'rm or by an', 'ans electro', 'mechanical ', 'ocopying re', 'ing or othe', 'e without w', 'en permissi', 'f the publi', ' for inform', 'n regarding', 'missions wr', 'to scholast', 'nc attentio', 'rmissions d', 'tment  broa', ' new york n', 'brary of co', 'ss catalogi']\n",
            "['er and the ', 'erers stone', 'k rowlingil', 'rations by ', ' grandprart', 'a levine bo', 'n imprint o', 'holastic pr', 'or jessica ', 'loves stori', 'r anne who ', 'd them tooa', 'or di who h', ' this one f', 'text copyri', '  by jk row', 'illustratio', 'y mary gran', 'copyright  ', 'ner brosall', 'hts reserve', 'blished by ', 'lastic pres', 'division of', 'olastic inc', 'ishers sinc', 'holastic sc', 'stic press ', 'the lantern', 'oare tradem', ' andor regi', 'ed trademar', 'f scholasti', 'charry pott', 'nd all rela', 'characters ', 'elements ar', 'ademarks of', 'ner brosno ', ' of this pu', 'ation may b', 'produced or', 'red in a re', 'val system ', 'ransmitted ', 'ny form or ', 'ny means el', 'onic mechan', ' photocopyi', 'ecording or', 'erwise with', 'written per', 'ion of the ', 'isher for i', 'mation rega', 'g permissio', 'rite to sch', 'tic inc att', 'on permissi', 'department ', 'adway new y', 'ny library ', 'ongress cat', 'inginpublic']\n",
            "['nd']\n",
            "['d ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8D13l4n7FXT"
      },
      "source": [
        "def logprob(predictions, labels):\n",
        "    \"\"\"\n",
        "        Log-probability of the true labels in a predicted batch.\n",
        "    \"\"\"\n",
        "    predictions[predictions < 1e-10] = 1e-10\n",
        "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
        "\n",
        "def sample_distribution(distribution):\n",
        "    \"\"\"\n",
        "        Sample one element from a distribution assumed to be an array of normalized\n",
        "        probabilities.\n",
        "    \"\"\"\n",
        "    r = random.uniform(0, 1)\n",
        "    s = 0\n",
        "    for i in range(len(distribution)):\n",
        "        s += distribution[i]\n",
        "        if s >= r:\n",
        "            return i\n",
        "    return len(distribution) - 1\n",
        "\n",
        "def sample(prediction):\n",
        "    \"\"\"\n",
        "        Turn a (column) prediction into 1-hot encoded samples\n",
        "    \"\"\"\n",
        "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
        "    p[0, sample_distribution(prediction[0])] = 1.0\n",
        "    return p\n",
        "\n",
        "def random_distribution():\n",
        "    \"\"\"\n",
        "        Generate a random column of probabilities\n",
        "    \"\"\"\n",
        "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
        "    return b / np.sum(b, 1)[:, None]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoJyMoWK7FXU"
      },
      "source": [
        "num_nodes = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    # Parameters:\n",
        "    # Input gate: input, previous output, bias\n",
        "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Forget gate: input, previous output, bias\n",
        "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Memory cell: input, previous output, bias\n",
        "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Output gate: input, previous output, bias\n",
        "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Variables saving state across unrollings.\n",
        "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "    \n",
        "    # Classifier weights and biases\n",
        "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
        "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "    \n",
        "    # Define cell computation\n",
        "    def lstm_cell(i, o, state):\n",
        "        \"\"\"\n",
        "            Create LSTM cell.\n",
        "        \"\"\"\n",
        "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
        "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
        "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
        "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
        "        return output_gate * tf.tanh(state), state\n",
        "    \n",
        "    # Input data\n",
        "    train_data = list()\n",
        "    for _ in range(num_unrollings + 1):\n",
        "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
        "    train_inputs = train_data[:num_unrollings]\n",
        "    train_labels = train_data[1:]\n",
        "    \n",
        "    # Unrolled LSTM loop\n",
        "    outputs = list()\n",
        "    output = saved_output\n",
        "    state = saved_state\n",
        "    for i in train_inputs:\n",
        "        output, state = lstm_cell(i, output, state)\n",
        "        outputs.append(output)\n",
        "        \n",
        "    # State saving across unrollings\n",
        "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
        "        # Classifier\n",
        "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = tf.concat(train_labels, 0),\n",
        "                                                                      logits = logits))\n",
        "    \n",
        "    # Optimizer\n",
        "    global_step = tf.Variable(0)\n",
        "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
        "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
        "    \n",
        "    # Predictions\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    \n",
        "    # Sampling and validation eval: batch 1, no unrolling\n",
        "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
        "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    reset_sample_state = tf.group(\n",
        "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
        "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
        "    )\n",
        "    sample_output, sample_state = lstm_cell(sample_input, \n",
        "                                            saved_sample_output, \n",
        "                                            saved_sample_state)\n",
        "    \n",
        "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
        "                                  saved_sample_state.assign(sample_state)]):\n",
        "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXc2oiM17FXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529beeea-7990-4465-d038-2e1313abd330"
      },
      "source": [
        "num_steps = 20000\n",
        "summary_frequency = 100\n",
        "\n",
        "with tf.Session(graph = graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print(\"Initialized\")\n",
        "    \n",
        "    print(\"Training weights on Text8\")\n",
        "    # Train on Text8\n",
        "    mean_loss = 0\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        batches = train_batches.next()\n",
        "        feed_dict = dict()\n",
        "        \n",
        "        for i in range(num_unrollings + 1):\n",
        "            feed_dict[train_data[i]] = batches[i]\n",
        "            \n",
        "        _, l, predictions, lr = session.run(\n",
        "            [optimizer, loss, train_prediction, learning_rate], feed_dict = feed_dict)\n",
        "        \n",
        "        mean_loss += l\n",
        "        \n",
        "        if step % summary_frequency == 0:\n",
        "            if step > 0:\n",
        "                mean_loss = mean_loss / summary_frequency\n",
        "                \n",
        "            print(\"Average loss at step %d: %f learning rate: %f\" % (step, mean_loss, lr))\n",
        "            \n",
        "            mean_loss = 0\n",
        "            \n",
        "            labels = np.concatenate(list(batches)[1:])\n",
        "            print(\"Minibatch perplexity: %.2f\" % float(\n",
        "                np.exp(logprob(predictions, labels))\n",
        "            ))\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\"Tunining weights on Harry Potter\")\n",
        "    # Tune on Harry Potter\n",
        "    \n",
        "    mean_loss = 0\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        batches = tune_batches.next()\n",
        "        feed_dict = dict()\n",
        "        \n",
        "        for i in range(num_unrollings + 1):\n",
        "            feed_dict[train_data[i]] = batches[i]\n",
        "            \n",
        "        _, l, predictions, lr = session.run(\n",
        "            [optimizer, loss, train_prediction, learning_rate], feed_dict = feed_dict)\n",
        "        \n",
        "        mean_loss += l\n",
        "        \n",
        "        if step % summary_frequency == 0:\n",
        "            if step > 0:\n",
        "                mean_loss = mean_loss / summary_frequency\n",
        "                \n",
        "            print(\"Average loss at step %d: %f learning rate: %f\" % (step, mean_loss, lr))\n",
        "            \n",
        "            mean_loss = 0\n",
        "            \n",
        "            labels = np.concatenate(list(batches)[1:])\n",
        "            print(\"Minibatch perplexity: %.2f\" % float(\n",
        "                np.exp(logprob(predictions, labels))\n",
        "            ))\n",
        "            \n",
        "            if step % (summary_frequency * 10) == 0:\n",
        "                # Generate some samples\n",
        "                print('=' * 80)\n",
        "                for _ in range(5):\n",
        "                    feed = sample(random_distribution())\n",
        "                    sentence = characters(feed)[0]\n",
        "                    reset_sample_state.run()\n",
        "                    for _ in range(79):\n",
        "                        prediction = sample_prediction.eval({sample_input: feed})\n",
        "                        feed = sample(prediction)\n",
        "                        sentence += characters(feed)[0]\n",
        "                    print(sentence)\n",
        "                print('=' * 80)\n",
        "            # Mesaure validation set perplexity\n",
        "            reset_sample_state.run()\n",
        "            valid_logprob = 0\n",
        "            for _ in range(valid_size):\n",
        "                b = valid_batches.next()\n",
        "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
        "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
        "            print(\"Validation set perplexity: %.2f\" % float(np.exp(valid_logprob / valid_size)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Training weights on Text8\n",
            "Average loss at step 0: 3.294022 learning rate: 10.000000\n",
            "Minibatch perplexity: 26.95\n",
            "Average loss at step 100: 2.591139 learning rate: 10.000000\n",
            "Minibatch perplexity: 11.54\n",
            "Average loss at step 200: 2.261024 learning rate: 10.000000\n",
            "Minibatch perplexity: 9.23\n",
            "Average loss at step 300: 2.103871 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.86\n",
            "Average loss at step 400: 2.014105 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.68\n",
            "Average loss at step 500: 1.945172 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.31\n",
            "Average loss at step 600: 1.911439 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.87\n",
            "Average loss at step 700: 1.869281 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.25\n",
            "Average loss at step 800: 1.837627 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.33\n",
            "Average loss at step 900: 1.811218 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.81\n",
            "Average loss at step 1000: 1.830704 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.50\n",
            "Average loss at step 1100: 1.806868 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.31\n",
            "Average loss at step 1200: 1.753567 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.20\n",
            "Average loss at step 1300: 1.738377 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.40\n",
            "Average loss at step 1400: 1.739140 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.15\n",
            "Average loss at step 1500: 1.759721 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.86\n",
            "Average loss at step 1600: 1.729745 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.57\n",
            "Average loss at step 1700: 1.722745 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.54\n",
            "Average loss at step 1800: 1.698563 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.55\n",
            "Average loss at step 1900: 1.657302 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.64\n",
            "Average loss at step 2000: 1.673873 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.47\n",
            "Average loss at step 2100: 1.693404 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.65\n",
            "Average loss at step 2200: 1.684381 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.83\n",
            "Average loss at step 2300: 1.672937 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.92\n",
            "Average loss at step 2400: 1.650810 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.14\n",
            "Average loss at step 2500: 1.676710 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.81\n",
            "Average loss at step 2600: 1.649719 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.45\n",
            "Average loss at step 2700: 1.669563 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.43\n",
            "Average loss at step 2800: 1.638860 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.18\n",
            "Average loss at step 2900: 1.647510 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.46\n",
            "Average loss at step 3000: 1.659202 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.96\n",
            "Average loss at step 3100: 1.639399 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.39\n",
            "Average loss at step 3200: 1.639116 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.71\n",
            "Average loss at step 3300: 1.637032 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.33\n",
            "Average loss at step 3400: 1.645994 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.22\n",
            "Average loss at step 3500: 1.668327 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.34\n",
            "Average loss at step 3600: 1.674101 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.99\n",
            "Average loss at step 3700: 1.651128 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.83\n",
            "Average loss at step 3800: 1.647407 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.24\n",
            "Average loss at step 3900: 1.638959 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.69\n",
            "Average loss at step 4000: 1.649196 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.78\n",
            "Average loss at step 4100: 1.628224 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.41\n",
            "Average loss at step 4200: 1.638843 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.91\n",
            "Average loss at step 4300: 1.624605 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.39\n",
            "Average loss at step 4400: 1.614901 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.67\n",
            "Average loss at step 4500: 1.630677 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.59\n",
            "Average loss at step 4600: 1.602553 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.99\n",
            "Average loss at step 4700: 1.623345 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.78\n",
            "Average loss at step 4800: 1.629498 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.81\n",
            "Average loss at step 4900: 1.630932 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.75\n",
            "Average loss at step 5000: 1.614570 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.41\n",
            "Average loss at step 5100: 1.599064 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.84\n",
            "Average loss at step 5200: 1.604112 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.25\n",
            "Average loss at step 5300: 1.590851 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.83\n",
            "Average loss at step 5400: 1.578287 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.13\n",
            "Average loss at step 5500: 1.577626 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.02\n",
            "Average loss at step 5600: 1.572634 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 5700: 1.573255 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.59\n",
            "Average loss at step 5800: 1.580756 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.45\n",
            "Average loss at step 5900: 1.574547 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.05\n",
            "Average loss at step 6000: 1.551436 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.54\n",
            "Average loss at step 6100: 1.569143 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.05\n",
            "Average loss at step 6200: 1.555861 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.56\n",
            "Average loss at step 6300: 1.548468 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.82\n",
            "Average loss at step 6400: 1.531932 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.48\n",
            "Average loss at step 6500: 1.555071 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.86\n",
            "Average loss at step 6600: 1.584656 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.79\n",
            "Average loss at step 6700: 1.580646 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.98\n",
            "Average loss at step 6800: 1.601297 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.33\n",
            "Average loss at step 6900: 1.582345 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.72\n",
            "Average loss at step 7000: 1.595041 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.14\n",
            "Average loss at step 7100: 1.583584 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.38\n",
            "Average loss at step 7200: 1.570938 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.07\n",
            "Average loss at step 7300: 1.575926 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.91\n",
            "Average loss at step 7400: 1.574158 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.11\n",
            "Average loss at step 7500: 1.596509 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.09\n",
            "Average loss at step 7600: 1.570863 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.21\n",
            "Average loss at step 7700: 1.549671 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.66\n",
            "Average loss at step 7800: 1.577464 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.63\n",
            "Average loss at step 7900: 1.576799 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.44\n",
            "Average loss at step 8000: 1.601515 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.23\n",
            "Average loss at step 8100: 1.621353 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.01\n",
            "Average loss at step 8200: 1.574365 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.97\n",
            "Average loss at step 8300: 1.572420 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.52\n",
            "Average loss at step 8400: 1.575791 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.72\n",
            "Average loss at step 8500: 1.599544 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.43\n",
            "Average loss at step 8600: 1.579940 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.92\n",
            "Average loss at step 8700: 1.586649 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.83\n",
            "Average loss at step 8800: 1.552126 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.51\n",
            "Average loss at step 8900: 1.552042 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.17\n",
            "Average loss at step 9000: 1.555044 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.91\n",
            "Average loss at step 9100: 1.564501 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.99\n",
            "Average loss at step 9200: 1.585441 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.02\n",
            "Average loss at step 9300: 1.595021 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.15\n",
            "Average loss at step 9400: 1.599031 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.01\n",
            "Average loss at step 9500: 1.592836 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.91\n",
            "Average loss at step 9600: 1.578220 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.99\n",
            "Average loss at step 9700: 1.606494 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.68\n",
            "Average loss at step 9800: 1.600078 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.49\n",
            "Average loss at step 9900: 1.600822 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.20\n",
            "Average loss at step 10000: 1.604348 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.14\n",
            "Average loss at step 10100: 1.598743 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.10\n",
            "Average loss at step 10200: 1.573053 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.69\n",
            "Average loss at step 10300: 1.576857 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.89\n",
            "Average loss at step 10400: 1.585397 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.47\n",
            "Average loss at step 10500: 1.604954 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.27\n",
            "Average loss at step 10600: 1.609722 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.28\n",
            "Average loss at step 10700: 1.608302 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.99\n",
            "Average loss at step 10800: 1.616808 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.10\n",
            "Average loss at step 10900: 1.606415 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.73\n",
            "Average loss at step 11000: 1.592686 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.61\n",
            "Average loss at step 11100: 1.588383 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.72\n",
            "Average loss at step 11200: 1.581407 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 11300: 1.568531 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.67\n",
            "Average loss at step 11400: 1.590823 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.01\n",
            "Average loss at step 11500: 1.598417 learning rate: 0.100000\n",
            "Minibatch perplexity: 6.41\n",
            "Average loss at step 11600: 1.590766 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.28\n",
            "Average loss at step 11700: 1.608229 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.88\n",
            "Average loss at step 11800: 1.589896 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.50\n",
            "Average loss at step 11900: 1.588020 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.77\n",
            "Average loss at step 12000: 1.591571 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.77\n",
            "Average loss at step 12100: 1.586294 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.42\n",
            "Average loss at step 12200: 1.579547 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.95\n",
            "Average loss at step 12300: 1.587413 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.80\n",
            "Average loss at step 12400: 1.581419 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.10\n",
            "Average loss at step 12500: 1.595828 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.60\n",
            "Average loss at step 12600: 1.574392 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.75\n",
            "Average loss at step 12700: 1.565740 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.15\n",
            "Average loss at step 12800: 1.587282 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.72\n",
            "Average loss at step 12900: 1.573144 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.35\n",
            "Average loss at step 13000: 1.586743 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.75\n",
            "Average loss at step 13100: 1.583721 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.23\n",
            "Average loss at step 13200: 1.575080 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 13300: 1.596561 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.15\n",
            "Average loss at step 13400: 1.568764 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 13500: 1.544848 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.77\n",
            "Average loss at step 13600: 1.587975 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.60\n",
            "Average loss at step 13700: 1.599696 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.81\n",
            "Average loss at step 13800: 1.574535 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.54\n",
            "Average loss at step 13900: 1.578906 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.16\n",
            "Average loss at step 14000: 1.568186 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.98\n",
            "Average loss at step 14100: 1.597426 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.71\n",
            "Average loss at step 14200: 1.603905 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.54\n",
            "Average loss at step 14300: 1.543571 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.82\n",
            "Average loss at step 14400: 1.560340 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.12\n",
            "Average loss at step 14500: 1.588979 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.92\n",
            "Average loss at step 14600: 1.585123 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.11\n",
            "Average loss at step 14700: 1.583839 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.91\n",
            "Average loss at step 14800: 1.592891 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.07\n",
            "Average loss at step 14900: 1.615421 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.03\n",
            "Average loss at step 15000: 1.570862 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.31\n",
            "Average loss at step 15100: 1.554530 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.95\n",
            "Average loss at step 15200: 1.583355 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.25\n",
            "Average loss at step 15300: 1.571164 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.73\n",
            "Average loss at step 15400: 1.572688 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.70\n",
            "Average loss at step 15500: 1.576677 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.84\n",
            "Average loss at step 15600: 1.566909 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.73\n",
            "Average loss at step 15700: 1.602176 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.95\n",
            "Average loss at step 15800: 1.597133 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.84\n",
            "Average loss at step 15900: 1.582160 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.88\n",
            "Average loss at step 16000: 1.584707 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.03\n",
            "Average loss at step 16100: 1.620701 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.21\n",
            "Average loss at step 16200: 1.571770 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.61\n",
            "Average loss at step 16300: 1.562323 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.58\n",
            "Average loss at step 16400: 1.603682 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.33\n",
            "Average loss at step 16500: 1.608303 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.64\n",
            "Average loss at step 16600: 1.599072 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.17\n",
            "Average loss at step 16700: 1.602690 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.11\n",
            "Average loss at step 16800: 1.599780 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.00\n",
            "Average loss at step 16900: 1.590485 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.71\n",
            "Average loss at step 17000: 1.578581 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.05\n",
            "Average loss at step 17100: 1.592885 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.85\n",
            "Average loss at step 17200: 1.575502 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.77\n",
            "Average loss at step 17300: 1.585997 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.75\n",
            "Average loss at step 17400: 1.574890 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.13\n",
            "Average loss at step 17500: 1.576942 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.02\n",
            "Average loss at step 17600: 1.576518 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.15\n",
            "Average loss at step 17700: 1.590328 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.70\n",
            "Average loss at step 17800: 1.591863 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.11\n",
            "Average loss at step 17900: 1.569728 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.50\n",
            "Average loss at step 18000: 1.582939 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.54\n",
            "Average loss at step 18100: 1.589637 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.89\n",
            "Average loss at step 18200: 1.581838 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.30\n",
            "Average loss at step 18300: 1.561247 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.83\n",
            "Average loss at step 18400: 1.574588 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.94\n",
            "Average loss at step 18500: 1.605774 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.92\n",
            "Average loss at step 18600: 1.607935 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.61\n",
            "Average loss at step 18700: 1.592787 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.07\n",
            "Average loss at step 18800: 1.564571 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.55\n",
            "Average loss at step 18900: 1.574187 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.52\n",
            "Average loss at step 19000: 1.591059 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.99\n",
            "Average loss at step 19100: 1.623343 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.95\n",
            "Average loss at step 19200: 1.618950 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.68\n",
            "Average loss at step 19300: 1.604539 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.87\n",
            "Average loss at step 19400: 1.593891 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.75\n",
            "Average loss at step 19500: 1.570476 learning rate: 0.010000\n",
            "Minibatch perplexity: 3.93\n",
            "Average loss at step 19600: 1.614370 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.86\n",
            "Average loss at step 19700: 1.597687 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.87\n",
            "Average loss at step 19800: 1.570115 learning rate: 0.010000\n",
            "Minibatch perplexity: 4.65\n",
            "Average loss at step 19900: 1.599839 learning rate: 0.010000\n",
            "Minibatch perplexity: 5.08\n",
            "Tunining weights on Harry Potter\n",
            "Average loss at step 0: 2.243984 learning rate: 0.001000\n",
            "Minibatch perplexity: 9.43\n",
            "================================================================================\n",
            "inccism repriss of cocux basuc to later of thearlia viliands material order mari\n",
            "formans action that and spost panetayed two general shown take deneres hald old \n",
            "r in in u had amstralta between the state considered brunch and wah american out\n",
            "uminanc basipn marey location one one five bit dury two zero zero one zero inter\n",
            "ing as maniprage billthanatories grimate k panages german regients y primates of\n",
            "================================================================================\n",
            "Validation set perplexity: 7.56\n",
            "Average loss at step 100: 1.900959 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.57\n",
            "Validation set perplexity: 6.35\n",
            "Average loss at step 200: 1.897640 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.54\n",
            "Validation set perplexity: 7.14\n",
            "Average loss at step 300: 1.894334 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.52\n",
            "Validation set perplexity: 6.68\n",
            "Average loss at step 400: 1.891060 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.50\n",
            "Validation set perplexity: 6.55\n",
            "Average loss at step 500: 1.887818 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.48\n",
            "Validation set perplexity: 6.38\n",
            "Average loss at step 600: 1.884609 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.45\n",
            "Validation set perplexity: 6.87\n",
            "Average loss at step 700: 1.881433 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.43\n",
            "Validation set perplexity: 6.44\n",
            "Average loss at step 800: 1.878291 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.41\n",
            "Validation set perplexity: 6.19\n",
            "Average loss at step 900: 1.875184 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.39\n",
            "Validation set perplexity: 8.39\n",
            "Average loss at step 1000: 1.872112 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.37\n",
            "================================================================================\n",
            "fliewa old after tepritive induside early icheasure the preductions activity con\n",
            "coments to priicary resabury district in stationdingey regalore thove hoand when\n",
            "n k unarped processane politations movies source an in the cholephomae is furthe\n",
            "t one five of stacting the bebon given use the one nine eight five one five thre\n",
            "xism cass alblanessial selve woldedre perioderratould at at norke public of the \n",
            "================================================================================\n",
            "Validation set perplexity: 7.58\n",
            "Average loss at step 1100: 1.869075 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.35\n",
            "Validation set perplexity: 6.71\n",
            "Average loss at step 1200: 1.866073 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.33\n",
            "Validation set perplexity: 6.32\n",
            "Average loss at step 1300: 1.863107 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.31\n",
            "Validation set perplexity: 7.01\n",
            "Average loss at step 1400: 1.860174 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.29\n",
            "Validation set perplexity: 7.50\n",
            "Average loss at step 1500: 1.857276 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.27\n",
            "Validation set perplexity: 7.77\n",
            "Average loss at step 1600: 1.854410 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.25\n",
            "Validation set perplexity: 7.45\n",
            "Average loss at step 1700: 1.851577 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.23\n",
            "Validation set perplexity: 8.64\n",
            "Average loss at step 1800: 1.848774 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.21\n",
            "Validation set perplexity: 6.91\n",
            "Average loss at step 1900: 1.846002 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.20\n",
            "Validation set perplexity: 6.72\n",
            "Average loss at step 2000: 1.843259 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.18\n",
            "================================================================================\n",
            "fro strangs the one nine four us of celpiced for ride irhout sensance which thle\n",
            "y far parin actions amenending though of then feghte and spow toward m is the ar\n",
            "ble thus than the foundon to cr essism to of dettemmetion requires norms united \n",
            "k article at lead on absolged card one nine six nine one fronkslam in she an mov\n",
            "by other meltanical mart of short de planfmexed numanning a southeramy it for po\n",
            "================================================================================\n",
            "Validation set perplexity: 6.83\n",
            "Average loss at step 2100: 1.840545 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.16\n",
            "Validation set perplexity: 8.42\n",
            "Average loss at step 2200: 1.837858 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.14\n",
            "Validation set perplexity: 8.03\n",
            "Average loss at step 2300: 1.835197 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.13\n",
            "Validation set perplexity: 7.10\n",
            "Average loss at step 2400: 1.832563 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.11\n",
            "Validation set perplexity: 6.58\n",
            "Average loss at step 2500: 1.829954 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.09\n",
            "Validation set perplexity: 6.22\n",
            "Average loss at step 2600: 1.827370 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.08\n",
            "Validation set perplexity: 6.71\n",
            "Average loss at step 2700: 1.824810 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.06\n",
            "Validation set perplexity: 6.60\n",
            "Average loss at step 2800: 1.822273 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.04\n",
            "Validation set perplexity: 6.86\n",
            "Average loss at step 2900: 1.819759 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.03\n",
            "Validation set perplexity: 7.33\n",
            "Average loss at step 3000: 1.817268 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.01\n",
            "================================================================================\n",
            "quement rather part tockise batters gonsasks starges londas posstern comminist p\n",
            "vihuzare one nine two pla demanumes two zero zero places and helless yoz creicy \n",
            "way gnils and millionals apcripel communially centrellege long regorul links the\n",
            "cles of the notlees and to pajsion was offly technority participtive deach regai\n",
            " spice prograpity and strimens was antier computine as officianly quarch dehils \n",
            "================================================================================\n",
            "Validation set perplexity: 7.81\n",
            "Average loss at step 3100: 1.814798 learning rate: 0.001000\n",
            "Minibatch perplexity: 6.00\n",
            "Validation set perplexity: 7.85\n",
            "Average loss at step 3200: 1.812350 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.98\n",
            "Validation set perplexity: 7.87\n",
            "Average loss at step 3300: 1.809923 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.97\n",
            "Validation set perplexity: 6.97\n",
            "Average loss at step 3400: 1.807517 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.95\n",
            "Validation set perplexity: 6.29\n",
            "Average loss at step 3500: 1.805131 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.94\n",
            "Validation set perplexity: 7.49\n",
            "Average loss at step 3600: 1.802765 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.92\n",
            "Validation set perplexity: 6.48\n",
            "Average loss at step 3700: 1.800419 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.91\n",
            "Validation set perplexity: 6.82\n",
            "Average loss at step 3800: 1.798091 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.89\n",
            "Validation set perplexity: 7.51\n",
            "Average loss at step 3900: 1.795783 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.88\n",
            "Validation set perplexity: 7.84\n",
            "Average loss at step 4000: 1.793493 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.87\n",
            "================================================================================\n",
            "difiversed to do been and bursaliraps form may frencher persiots lieit pack whic\n",
            "que one nine five nine however ss our r defendine and o yasse balk they august p\n",
            "k the usuelatleves and the fight harbard crost politic of that religious on thor\n",
            "work one nine five republished sungs borb proprin and that tendin were regispira\n",
            "ting ismitran concempord in autumbe all stremity superfing this they other consi\n",
            "================================================================================\n",
            "Validation set perplexity: 7.07\n",
            "Average loss at step 4100: 1.791221 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.85\n",
            "Validation set perplexity: 6.37\n",
            "Average loss at step 4200: 1.788967 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.84\n",
            "Validation set perplexity: 6.42\n",
            "Average loss at step 4300: 1.786731 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.83\n",
            "Validation set perplexity: 6.27\n",
            "Average loss at step 4400: 1.784511 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.81\n",
            "Validation set perplexity: 7.18\n",
            "Average loss at step 4500: 1.782309 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.80\n",
            "Validation set perplexity: 7.13\n",
            "Average loss at step 4600: 1.780124 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.79\n",
            "Validation set perplexity: 7.54\n",
            "Average loss at step 4700: 1.777955 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.77\n",
            "Validation set perplexity: 6.95\n",
            "Average loss at step 4800: 1.775802 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.76\n",
            "Validation set perplexity: 7.40\n",
            "Average loss at step 4900: 1.773665 learning rate: 0.001000\n",
            "Minibatch perplexity: 5.75\n",
            "Validation set perplexity: 7.62\n",
            "Average loss at step 5000: 1.771544 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "================================================================================\n",
            "ai gelmory coint ray diftro tratbling sh nine ofcerware lifined non caza order c\n",
            "ques one seven eight zero zero eightherner on not ising that the three rehower p\n",
            "tical and use and period of in films also two meach this was not trit is and app\n",
            "one number greego accord they also king after the marticle s pholegues with anti\n",
            "z empire respitual newsoxy the numagel ormediad with itsolded tyles it on locan \n",
            "================================================================================\n",
            "Validation set perplexity: 7.66\n",
            "Average loss at step 5100: 1.770389 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 7.07\n",
            "Average loss at step 5200: 1.770179 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 6.74\n",
            "Average loss at step 5300: 1.769970 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 6.98\n",
            "Average loss at step 5400: 1.769760 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 6.77\n",
            "Average loss at step 5500: 1.769550 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 6.75\n",
            "Average loss at step 5600: 1.769341 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 6.85\n",
            "Average loss at step 5700: 1.769132 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.73\n",
            "Validation set perplexity: 6.50\n",
            "Average loss at step 5800: 1.768923 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 7.25\n",
            "Average loss at step 5900: 1.768714 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 6.78\n",
            "Average loss at step 6000: 1.768505 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "================================================================================\n",
            "wishte rayer of the nistises former clusions to alacle in a free starpehs transe\n",
            "lee meass is exrovernmy altrisent carnes from that included hurring when a bound\n",
            "y the or efn two zero zero zero one nine zero to viditional facled useigy since \n",
            "quesripation this meri glive seumal parant and every prover of and nine other re\n",
            "reminet their the erics further was streets per badd these again a mosactic dymb\n",
            "================================================================================\n",
            "Validation set perplexity: 6.88\n",
            "Average loss at step 6100: 1.768296 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 6.65\n",
            "Average loss at step 6200: 1.768088 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 6.93\n",
            "Average loss at step 6300: 1.767880 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 7.33\n",
            "Average loss at step 6400: 1.767671 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 8.92\n",
            "Average loss at step 6500: 1.767463 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 7.67\n",
            "Average loss at step 6600: 1.767256 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 7.55\n",
            "Average loss at step 6700: 1.767048 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 9.12\n",
            "Average loss at step 6800: 1.766840 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 8.22\n",
            "Average loss at step 6900: 1.766633 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 8.79\n",
            "Average loss at step 7000: 1.766426 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "================================================================================\n",
            "yning from highwels acamer this they mecriantu conside dicketf depisebing forrya\n",
            "les are dairledises to sub the and for that to of the in the tarked rule s issur\n",
            "retions not a of lectusty jehnalt spinduny does sucker overscip mid one nine six\n",
            "war importiction of sh batha sparactert for dynerady two zero zero two bilitalin\n",
            "wari at the protainal and istorting point their scistifical to the graphanc avou\n",
            "================================================================================\n",
            "Validation set perplexity: 8.56\n",
            "Average loss at step 7100: 1.766218 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 8.33\n",
            "Average loss at step 7200: 1.766011 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 6.98\n",
            "Average loss at step 7300: 1.765805 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.71\n",
            "Validation set perplexity: 8.41\n",
            "Average loss at step 7400: 1.765598 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 8.43\n",
            "Average loss at step 7500: 1.765391 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 8.91\n",
            "Average loss at step 7600: 1.765185 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 7.05\n",
            "Average loss at step 7700: 1.764979 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 7.62\n",
            "Average loss at step 7800: 1.764773 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 7.17\n",
            "Average loss at step 7900: 1.764567 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 7.80\n",
            "Average loss at step 8000: 1.764361 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "================================================================================\n",
            "land from vistorigy about in the marniurly andwlyhispoce as this araraim the m s\n",
            "y is begr t gograble with any asslablt corrammes linvoss didey brughing inconals\n",
            "gro proposethions prinuit marting figbly dring doidish with one nine nine there \n",
            "gropy image or recearian as considered a duthminlinated that one nine eight thre\n",
            "ance wasle isen high also sefory the cooverges onlys of three margagent game fro\n",
            "================================================================================\n",
            "Validation set perplexity: 7.92\n",
            "Average loss at step 8100: 1.764155 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 9.44\n",
            "Average loss at step 8200: 1.763950 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 8.98\n",
            "Average loss at step 8300: 1.763744 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 8.10\n",
            "Average loss at step 8400: 1.763539 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 9.34\n",
            "Average loss at step 8500: 1.763334 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 8.55\n",
            "Average loss at step 8600: 1.763129 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 7.56\n",
            "Average loss at step 8700: 1.762924 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 7.76\n",
            "Average loss at step 8800: 1.762720 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 7.16\n",
            "Average loss at step 8900: 1.762515 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 7.69\n",
            "Average loss at step 9000: 1.762311 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "================================================================================\n",
            "viselintion many to his rock mary allower de head in the blove vohish of the hol\n",
            "zerous iradhy of a kithning sike when hoad noted dats the makes unitor words ord\n",
            "y two an it one nine nine a nf tle and liveno servic formon of that his watfel a\n",
            "one and medizinal fict seashiph opposed horby dy by amonch this the soup to dred\n",
            "kirs of as in evoumentline from whifinarchors house asphanthapa mod permeraton e\n",
            "================================================================================\n",
            "Validation set perplexity: 7.23\n",
            "Average loss at step 9100: 1.762107 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 6.87\n",
            "Average loss at step 9200: 1.761903 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 7.92\n",
            "Average loss at step 9300: 1.761699 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 8.98\n",
            "Average loss at step 9400: 1.761495 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 7.31\n",
            "Average loss at step 9500: 1.761291 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 7.30\n",
            "Average loss at step 9600: 1.761088 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 6.54\n",
            "Average loss at step 9700: 1.760884 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 7.61\n",
            "Average loss at step 9800: 1.760681 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.68\n",
            "Validation set perplexity: 7.58\n",
            "Average loss at step 9900: 1.760478 learning rate: 0.000100\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.74\n",
            "Average loss at step 10000: 1.760275 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "stake its unittro which sernuoqui adaiticly after sysfer rosing organ writthers \n",
            "cismmeses has one nine seven fine four dicture the which generaleys bringolopori\n",
            "ves constrantnon used work choilies of its howeved the gominathred diseasirised \n",
            "loset of nown engost to exclating gnt ny this famerantes to ovailarsmetion is fo\n",
            "dings for not contracts in onvers me site addent yum renfer by the became friend\n",
            "================================================================================\n",
            "Validation set perplexity: 6.91\n",
            "Average loss at step 10100: 1.760168 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.06\n",
            "Average loss at step 10200: 1.760154 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.48\n",
            "Average loss at step 10300: 1.760141 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.99\n",
            "Average loss at step 10400: 1.760127 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.23\n",
            "Average loss at step 10500: 1.760114 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.71\n",
            "Average loss at step 10600: 1.760101 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 9.70\n",
            "Average loss at step 10700: 1.760087 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.39\n",
            "Average loss at step 10800: 1.760074 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.85\n",
            "Average loss at step 10900: 1.760061 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.72\n",
            "Average loss at step 11000: 1.760047 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "e holes is domantings to logy seconar poth dessevharies wimf is a relingdount of\n",
            "al mar in louting but nioue computer that for the compation of righan links to a\n",
            "halidus ple fartin that singer the dewelt kaining stherowhon freed in groungrus \n",
            "latesion armaictian unimal the for one six night ed the five nine six one partic\n",
            "harts neararra spyriis prememalers prinobrath produced with the can corned thans\n",
            "================================================================================\n",
            "Validation set perplexity: 7.12\n",
            "Average loss at step 11100: 1.760034 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.09\n",
            "Average loss at step 11200: 1.760021 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.01\n",
            "Average loss at step 11300: 1.760007 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.00\n",
            "Average loss at step 11400: 1.759994 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.48\n",
            "Average loss at step 11500: 1.759981 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.78\n",
            "Average loss at step 11600: 1.759967 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.92\n",
            "Average loss at step 11700: 1.759954 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.47\n",
            "Average loss at step 11800: 1.759941 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.00\n",
            "Average loss at step 11900: 1.759927 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.50\n",
            "Average loss at step 12000: 1.759914 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "velted by sovered boites synts sort in the rerember florms and ded them with jis\n",
            "queng radiet fortis to the liphed only emon and mosterce mathalin sixter abm pro\n",
            "d precearce laydle start of who correct two zero zero zero houks in dijus govern\n",
            "tembok stording soved the long base on two zero white or stabltia subdynarated s\n",
            "ed and selmos hat af coadidaler on wust repeacts of cailsw classitions perfants \n",
            "================================================================================\n",
            "Validation set perplexity: 6.40\n",
            "Average loss at step 12100: 1.759901 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.13\n",
            "Average loss at step 12200: 1.759887 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.51\n",
            "Average loss at step 12300: 1.759874 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.85\n",
            "Average loss at step 12400: 1.759861 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.11\n",
            "Average loss at step 12500: 1.759847 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.03\n",
            "Average loss at step 12600: 1.759834 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.30\n",
            "Average loss at step 12700: 1.759821 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.64\n",
            "Average loss at step 12800: 1.759808 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.59\n",
            "Average loss at step 12900: 1.759794 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.28\n",
            "Average loss at step 13000: 1.759781 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "use also speakers your worg romeal molts bebout foundey whatsed to their akdoob \n",
            "graphys usurals manse ocumpa words a becarment in enrapacisted in occabed on the\n",
            "hasjement eddle serventary as thim partial latara a is properdy d fissible  vort\n",
            "sibe when herrey to engent rings one seven six three eight zero wledtion all is \n",
            "latedision for the microw againd through problold for its bermore s film in kaol\n",
            "================================================================================\n",
            "Validation set perplexity: 7.32\n",
            "Average loss at step 13100: 1.759768 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.53\n",
            "Average loss at step 13200: 1.759754 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.10\n",
            "Average loss at step 13300: 1.759741 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.80\n",
            "Average loss at step 13400: 1.759728 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.08\n",
            "Average loss at step 13500: 1.759714 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.70\n",
            "Average loss at step 13600: 1.759701 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.69\n",
            "Average loss at step 13700: 1.759688 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.11\n",
            "Average loss at step 13800: 1.759674 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.55\n",
            "Average loss at step 13900: 1.759661 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.73\n",
            "Average loss at step 14000: 1.759648 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "bir is bast is is dm across homes the related for anyly and and zaw distribles m\n",
            "alaryg decederative with baslianh such of the tippopulart variest most c such st\n",
            "rizon solation of population of this light smisica and are roman shararding land\n",
            "metiuss arf is bat zerf rerannes tayds bottany zero econular for the bring arege\n",
            "abilm word arded standing writerfly is nine one nine nine they od the century ze\n",
            "================================================================================\n",
            "Validation set perplexity: 7.23\n",
            "Average loss at step 14100: 1.759634 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.39\n",
            "Average loss at step 14200: 1.759621 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.85\n",
            "Average loss at step 14300: 1.759608 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.54\n",
            "Average loss at step 14400: 1.759594 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.92\n",
            "Average loss at step 14500: 1.759581 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.75\n",
            "Average loss at step 14600: 1.759568 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.86\n",
            "Average loss at step 14700: 1.759555 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.14\n",
            "Average loss at step 14800: 1.759541 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.29\n",
            "Average loss at step 14900: 1.759528 learning rate: 0.000010\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.89\n",
            "Average loss at step 15000: 1.759515 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "ques s patrical priikes gemeteral to two sectory set romandann into nines insite\n",
            "floul stativedural arrency at mustillom is the each such oc irelestry volees tea\n",
            "vet one new a storike fection first borary historking like from the crossem of s\n",
            "vied conder fiven creach and gionows seenamed abon dm main and its record strons\n",
            "zing used that two one eight many movormenymintter the supporter reseesshebicle \n",
            "================================================================================\n",
            "Validation set perplexity: 6.26\n",
            "Average loss at step 15100: 1.759508 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.58\n",
            "Average loss at step 15200: 1.759508 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.73\n",
            "Average loss at step 15300: 1.759508 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.25\n",
            "Average loss at step 15400: 1.759507 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 5.57\n",
            "Average loss at step 15500: 1.759507 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.11\n",
            "Average loss at step 15600: 1.759507 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 9.31\n",
            "Average loss at step 15700: 1.759507 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.61\n",
            "Average loss at step 15800: 1.759507 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.30\n",
            "Average loss at step 15900: 1.759506 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.95\n",
            "Average loss at step 16000: 1.759506 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "an as is villfanitical after hud val s unordentism must fathore astrundiph agp t\n",
            "lar mid umranhulism g on europiohics hypration powered the single liberate hadia\n",
            "vesson from the during against of the fromation viplani of stractive hasse nizht\n",
            "verynales gorgnal very that given haw aiple jacumant sisting poly mosterness isl\n",
            "urn that to gots strikor and the cozn others become of the ecsterncy estaber lis\n",
            "================================================================================\n",
            "Validation set perplexity: 7.19\n",
            "Average loss at step 16100: 1.759506 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.67\n",
            "Average loss at step 16200: 1.759506 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.62\n",
            "Average loss at step 16300: 1.759506 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.87\n",
            "Average loss at step 16400: 1.759505 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.30\n",
            "Average loss at step 16500: 1.759505 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.23\n",
            "Average loss at step 16600: 1.759505 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.19\n",
            "Average loss at step 16700: 1.759505 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.19\n",
            "Average loss at step 16800: 1.759505 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.09\n",
            "Average loss at step 16900: 1.759504 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.48\n",
            "Average loss at step 17000: 1.759504 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "k isonorlesmin allomis of the busy tooar were and states all attrover in prosis \n",
            "lesfled in the meam crityuss for suements researchy juny arthy in that ostive or\n",
            "chabertule was abbeienger of termales has for unitter lovel one six five each co\n",
            "arte fance four bazte azarograph function trieched as a showaze religils destact\n",
            "dicle one six three zero jay cabits englifit sommise perpotemetly the my two zer\n",
            "================================================================================\n",
            "Validation set perplexity: 6.58\n",
            "Average loss at step 17100: 1.759504 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.92\n",
            "Average loss at step 17200: 1.759504 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.42\n",
            "Average loss at step 17300: 1.759504 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.25\n",
            "Average loss at step 17400: 1.759503 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.27\n",
            "Average loss at step 17500: 1.759503 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.03\n",
            "Average loss at step 17600: 1.759503 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 5.62\n",
            "Average loss at step 17700: 1.759503 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.23\n",
            "Average loss at step 17800: 1.759503 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.38\n",
            "Average loss at step 17900: 1.759502 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.85\n",
            "Average loss at step 18000: 1.759502 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "s sarport amentures founded to its runshuwasoct larging gradulmant of the regard\n",
            "go often bindly mare thy spirt regiefly been linkage or the time two an american\n",
            "enteral about the electroins awas learthinct followicilation by madely their and\n",
            "gen generatoves or film by interbar toolics of the government and there roiono o\n",
            "frie used suckingra ministerust agratical ris martarawaardos two s controllogic \n",
            "================================================================================\n",
            "Validation set perplexity: 6.82\n",
            "Average loss at step 18100: 1.759502 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.31\n",
            "Average loss at step 18200: 1.759502 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.61\n",
            "Average loss at step 18300: 1.759502 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.30\n",
            "Average loss at step 18400: 1.759501 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.95\n",
            "Average loss at step 18500: 1.759501 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.21\n",
            "Average loss at step 18600: 1.759501 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.15\n",
            "Average loss at step 18700: 1.759501 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.05\n",
            "Average loss at step 18800: 1.759501 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.77\n",
            "Average loss at step 18900: 1.759501 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.90\n",
            "Average loss at step 19000: 1.759500 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "================================================================================\n",
            "jersland rults trandnetaining diran burbai breader z morgins of the avereal we t\n",
            "f sasualian government to the inbut that the founsing simple for s order minorit\n",
            "zack nd wordnan as relaised to excedicing the feel streeding essacting the molit\n",
            "speppol that sasphili rovel which apple consinies a zero one five nine zero just\n",
            "fictions in albuse ammuttifical government progrations and lusting the sarious d\n",
            "================================================================================\n",
            "Validation set perplexity: 7.54\n",
            "Average loss at step 19100: 1.759500 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.69\n",
            "Average loss at step 19200: 1.759500 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.88\n",
            "Average loss at step 19300: 1.759500 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.30\n",
            "Average loss at step 19400: 1.759500 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.66\n",
            "Average loss at step 19500: 1.759499 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.44\n",
            "Average loss at step 19600: 1.759499 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 8.70\n",
            "Average loss at step 19700: 1.759499 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.82\n",
            "Average loss at step 19800: 1.759499 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 6.77\n",
            "Average loss at step 19900: 1.759499 learning rate: 0.000001\n",
            "Minibatch perplexity: 5.67\n",
            "Validation set perplexity: 7.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3kEIACa7FXX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./Dataset/text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('./Dataset/text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size Text8 100000000\n",
      "Data size Harry Potter 5893451\n"
     ]
    }
   ],
   "source": [
    "def clean_character(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return char\n",
    "    elif char in string.ascii_uppercase:\n",
    "        return char.lower()\n",
    "    elif char == ' ':\n",
    "        return char\n",
    "    return ''\n",
    "\n",
    "def read_data(filename):\n",
    "    if os.path.splitext(filename)[1] == '.zip':\n",
    "        with zipfile.ZipFile(filename) as f:\n",
    "            name = f.namelist()[0]\n",
    "            data = tf.compat.as_str(f.read(name))\n",
    "    elif os.path.splitext(filename)[1] == '.txt':\n",
    "        with open(filename, 'r') as f:\n",
    "            data = f.readlines()\n",
    "            data = ''.join(data)\n",
    "    return data\n",
    "  \n",
    "text8 = read_data(filename)\n",
    "text8 = ''.join([clean_character(char) for char in text8])\n",
    "\n",
    "hp_path = \"./Dataset/HarryPotter/\"\n",
    "harry_potter = ''.join([read_data(filename) \n",
    "                        for filename in [os.path.join(hp_path, path) \n",
    "                                         for path in os.listdir(hp_path)]])\n",
    "harry_potter = ''.join([clean_character(char) for char in harry_potter])\n",
    "print('Data size Text8 %d' % len(text8))\n",
    "print('Data size Harry Potter %d' % len(harry_potter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001000  anarchism originated as a term of abuse first used against earl\n",
      "1000 nd the sorcerers stone  by jk rowlingp cmsummary rescued from th\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "train_text = text8\n",
    "tune_text = harry_potter[:valid_size]\n",
    "valid_text = harry_potter[valid_size:]\n",
    "train_size = len(train_text)\n",
    "tune_size = len(tune_text)\n",
    "print(train_size + tune_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anarchism ', 'ion from eu', 'esident lyn', 'r professio', ' minting af', 'concentrati', 'poken today', 'ber of hote', ' the bell a', 'overty with', 'anufacturer', ' britannica', 'd so david ', 'ea of worke', ' informatio', ' one hand a', 'hannel isla', 'ater reprin', 'r an invest', ' infinite a', 's explained', 'on of plant', ' be solved ', 'n nine resp', 'ation vital', 'self contai', 'ountless se', 'ce arag leg', 'are often c', 'ions eml th', 'ne such as ', 's livin thi', 'ecommunicat', ' one nine n', 'e six two z', ' computer g', 'ing to quan', 'erbicide to', ' special re', ' exotic per', 'rm indicati', 'a secular r', 'the frequen', ' synthesize', 'structural ', ' the media ', 'ail of tear', 'stems able ', 'anded by si', 'rophy assoc', ' union s pi', 'ired differ', 'hough not i', 'e minister ', 'y the king ', 'eudo histor', 'nounced at ', 'tics gullbe', 'sance compa', ' colleges w', 'et and sunr', 'e american ', 'he mass app', 'made such d']\n",
      "[' originated', 'urope aided', 'ndon johnso', 'on allowanc', 'fonso also ', 'ion camp wh', 'y as a firs', 'els and fin', 'aircraft fa', 'h the help ', 'rs such as ', 'a successio', ' collects t', 'ers self ma', 'on the meda', 'and removin', 'and french ', 'nted in boo', 'tigation of', 'amount of t', 'd above is ', 't shoots cr', ' and which ', 'pectively a', 'l for contr', 'ined system', 'exual perve', 'gal insuran', 'called in b', 'his is used', ' esoteric b', 'ing remade ', 'tions techn', 'nine two th', 'zero zero f', 'game afterb', 'ntum mechan', 'o the exten', 'elativity s', 'rfections t', 'ing that th', 'republic la', 'ncy r sqrt ', 'ed retrovir', ' reforms fa', ' announced ', 'rs the rise', ' to run on ', 'ir harry sm', 'ciation a c', 'iatiletka o', 'rent combin', 'itself guil', ' final year', ' serves a l', 'ry geoffrey', ' ricky s cl', 'erg jan mat', 'any and a m', 'which gradu', 'rise occur ', ' indian res', 'peal of dev', 'devices pos']\n",
      "['harry potte', 'd the sorce', ' stonebyj k', 'lingillustr', 'ns by mary ', 'dprarthur a', 'ine booksan', 'rint of sch', 'tic pressfo', 'ssica who l', ' storiesfor', 'e who loved', 'm tooand fo', ' who heard ', ' one firstt', 'copyright  ', 'jk rowlingi', 'trations by', 'y grandpr c', 'ight   warn', 'rosall righ', 'eserved pub', 'ed by schol', 'c press a d', 'ion of scho', 'ic incpubli', 's since sch', 'tic scholas', 'press and t', 'antern logo', 'trademarks ', 'r registere', 'ademarks of', 'olastic inc', 'y potter an', 'l related c', 'cters and e', 'nts are tra', 'rks of warn', 'rosno part ', 'his publica', ' may be rep', 'ced or stor', 'n a retriev', 'ystem or tr', 'itted in an', 'rm or by an', 'ans electro', 'mechanical ', 'ocopying re', 'ing or othe', 'e without w', 'en permissi', 'f the publi', ' for inform', 'n regarding', 'missions wr', 'to scholast', 'nc attentio', 'rmissions d', 'tment  broa', ' new york n', 'brary of co', 'ss catalogi']\n",
      "['er and the ', 'erers stone', 'k rowlingil', 'rations by ', ' grandprart', 'a levine bo', 'n imprint o', 'holastic pr', 'or jessica ', 'loves stori', 'r anne who ', 'd them tooa', 'or di who h', ' this one f', 'text copyri', '  by jk row', 'illustratio', 'y mary gran', 'copyright  ', 'ner brosall', 'hts reserve', 'blished by ', 'lastic pres', 'division of', 'olastic inc', 'ishers sinc', 'holastic sc', 'stic press ', 'the lantern', 'oare tradem', ' andor regi', 'ed trademar', 'f scholasti', 'charry pott', 'nd all rela', 'characters ', 'elements ar', 'ademarks of', 'ner brosno ', ' of this pu', 'ation may b', 'produced or', 'red in a re', 'val system ', 'ransmitted ', 'ny form or ', 'ny means el', 'onic mechan', ' photocopyi', 'ecording or', 'erwise with', 'written per', 'ion of the ', 'isher for i', 'mation rega', 'g permissio', 'rite to sch', 'tic inc att', 'on permissi', 'department ', 'adway new y', 'ny library ', 'ongress cat', 'inginpublic']\n",
      "['nd']\n",
      "['d ']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "            Generate the next array of batches from the data. The array consists of\n",
    "            the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"\n",
    "        Turn a 1-hot encoding or a probability distribution over the possible characters\n",
    "        back into its (most likely) character representation.\n",
    "    \"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"\n",
    "        Convert a sequence of batches back into their (most likely) string representation\n",
    "    \"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "tune_batches = BatchGenerator(tune_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(tune_batches.next()))\n",
    "print(batches2string(tune_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"\n",
    "        Log-probability of the true labels in a predicted batch.\n",
    "    \"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"\n",
    "        Sample one element from a distribution assumed to be an array of normalized\n",
    "        probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"\n",
    "        Turn a (column) prediction into 1-hot encoded samples\n",
    "    \"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"\n",
    "        Generate a random column of probabilities\n",
    "    \"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b / np.sum(b, 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-79-9b4316a71705>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, bias\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, bias\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, previous output, bias\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate: input, previous output, bias\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Define cell computation\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"\n",
    "            Create LSTM cell.\n",
    "        \"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]\n",
    "    \n",
    "    # Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    # State saving across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = tf.concat(train_labels, 0),\n",
    "                                                                      logits = logits))\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                            saved_sample_output, \n",
    "                                            saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Training weights on Text8\n",
      "Average loss at step 0: 3.295992 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "Average loss at step 50: 2.758502 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.39\n",
      "Average loss at step 100: 2.456948 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.06\n",
      "Average loss at step 150: 2.297194 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.06\n",
      "Average loss at step 200: 2.187658 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Average loss at step 250: 2.125565 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Average loss at step 300: 2.037266 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Average loss at step 350: 2.022811 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Average loss at step 400: 1.972956 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Average loss at step 450: 1.975365 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Average loss at step 500: 1.977211 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Average loss at step 550: 1.921242 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Average loss at step 600: 1.907067 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Average loss at step 650: 1.897807 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Average loss at step 700: 1.862766 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Average loss at step 750: 1.865021 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Average loss at step 800: 1.803314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 850: 1.780370 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Average loss at step 900: 1.764747 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Average loss at step 950: 1.772026 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Average loss at step 1000: 1.794021 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Average loss at step 1050: 1.800393 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Average loss at step 1100: 1.786941 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Average loss at step 1150: 1.781089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Average loss at step 1200: 1.777577 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Average loss at step 1250: 1.764030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Average loss at step 1300: 1.734795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Average loss at step 1350: 1.737565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 1400: 1.715811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 1450: 1.753398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Average loss at step 1500: 1.731103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 1550: 1.705387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Average loss at step 1600: 1.721657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Average loss at step 1650: 1.720505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 1700: 1.736435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Average loss at step 1750: 1.706832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Average loss at step 1800: 1.687557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Average loss at step 1850: 1.692316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Average loss at step 1900: 1.705686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Average loss at step 1950: 1.708060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Tunining weights on Harry Potter\n",
      "Average loss at step 0: 2.308306 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.06\n",
      "================================================================================\n",
      "ourthongrattrrection of to jotoratischissowsposcyelfred evented knowthickzd by f\n",
      "ding becamultspwils mordurchisic wrotesseghetrs a doullors one nine eight smarer\n",
      "wollies scholaldswarting finms one bork ddabtcist of more threen ka set ongino p\n",
      "welves in also c tendarss tincelidsmed of c fastivectornoty reforms succables mo\n",
      "bun agrices exering liber of in trabmlation runchale oriation arristaing butshoy\n",
      "================================================================================\n",
      "Validation set perplexity: 9.19\n",
      "Average loss at step 50: 0.822214 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.35\n",
      "Validation set perplexity: 25.28\n",
      "Average loss at step 100: 0.259950 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.12\n",
      "Validation set perplexity: 42.95\n",
      "Average loss at step 150: 0.075557 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 67.62\n",
      "Average loss at step 200: 0.129780 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.24\n",
      "Validation set perplexity: 72.27\n",
      "Average loss at step 250: 0.060032 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 136.81\n",
      "Average loss at step 300: 0.040650 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 155.25\n",
      "Average loss at step 350: 0.138331 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 114.08\n",
      "Average loss at step 400: 0.034398 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 176.33\n",
      "Average loss at step 450: 0.036442 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 144.85\n",
      "Average loss at step 500: 0.044532 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "================================================================================\n",
      "marry potter aharry potter and thattene beosla stede who headd this one firsttex\n",
      "vision revarding be divisions write to scholastic incharry potter and all belasi\n",
      "ch publication datarowling jkharry potter ahpyrssou charocoress catalowing recho\n",
      "vision of scholastic inc attention permissions write to scholastic inc attention\n",
      "marry porter aharry potter and all related characters and elements are trademark\n",
      "================================================================================\n",
      "Validation set perplexity: 173.11\n",
      "Average loss at step 550: 0.026674 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.01\n",
      "Validation set perplexity: 367.47\n",
      "Average loss at step 600: 0.164946 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 201.07\n",
      "Average loss at step 650: 0.029768 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 214.67\n",
      "Average loss at step 700: 0.037803 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 235.54\n",
      "Average loss at step 750: 0.064550 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 220.92\n",
      "Average loss at step 800: 0.029390 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 241.81\n",
      "Average loss at step 850: 0.034180 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "Validation set perplexity: 323.56\n",
      "Average loss at step 900: 0.062416 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.07\n",
      "Validation set perplexity: 275.26\n",
      "Average loss at step 950: 0.030438 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 472.36\n",
      "Average loss at step 1000: 0.034761 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "================================================================================\n",
      "warner brosall rights reserved published by scholastic press wastex by any means\n",
      "edmblosal inc whinival photocopy ligaticlustratious bo logareserresfing wrons be\n",
      "d this one firsttext copyright   warner brosall rights resered and marks andor r\n",
      " writtic scholastic press and the lantern logoare trademarks andor registered tr\n",
      "chussicpure who aurme ahess cotcexwark reserend  bara levine booksan impressfor \n",
      "================================================================================\n",
      "Validation set perplexity: 462.87\n",
      "Average loss at step 1050: 0.027233 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.18\n",
      "Validation set perplexity: 519.00\n",
      "Average loss at step 1100: 0.378728 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 235.39\n",
      "Average loss at step 1150: 0.033523 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 383.95\n",
      "Average loss at step 1200: 0.057570 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 286.96\n",
      "Average loss at step 1250: 0.046476 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 352.53\n",
      "Average loss at step 1300: 0.030596 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "Validation set perplexity: 431.77\n",
      "Average loss at step 1350: 0.065395 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 339.50\n",
      "Average loss at step 1400: 0.019609 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 472.92\n",
      "Average loss at step 1450: 0.087460 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 366.69\n",
      "Average loss at step 1500: 0.027376 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.03\n",
      "================================================================================\n",
      "qub sixchor anne who loved them tooand for foomica who loves stories of this pub\n",
      "htstent of scholastic inc attention permissions department  broadway new york ny\n",
      "d this one firsttext copyrigho  syhosass of scholastic press and the lantern log\n",
      "wibe yelated charyce wis a loved bhis andor resebsteal sy bl ann mean phyhouti w\n",
      "or by any meand this one firsttext copyright   by jk rowlingillustrations by mar\n",
      "================================================================================\n",
      "Validation set perplexity: 471.82\n",
      "Average loss at step 1550: 0.017741 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.08\n",
      "Validation set perplexity: 470.77\n",
      "Average loss at step 1600: 0.057955 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "Validation set perplexity: 372.30\n",
      "Average loss at step 1650: 0.029876 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "Validation set perplexity: 544.04\n",
      "Average loss at step 1700: 0.035040 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.32\n",
      "Validation set perplexity: 574.76\n",
      "Average loss at step 1750: 0.062962 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.02\n",
      "Validation set perplexity: 451.25\n",
      "Average loss at step 1800: 0.021816 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 594.81\n",
      "Average loss at step 1850: 0.038777 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 400.21\n",
      "Average loss at step 1900: 0.020451 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.01\n",
      "Validation set perplexity: 601.07\n",
      "Average loss at step 1950: 0.021859 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.10\n",
      "Validation set perplexity: 693.39\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2000\n",
    "summary_frequency = 50\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    print(\"Training weights on Text8\")\n",
    "    # Train on Text8\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict = feed_dict)\n",
    "        \n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                \n",
    "            print(\"Average loss at step %d: %f learning rate: %f\" % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print(\"Minibatch perplexity: %.2f\" % float(\n",
    "                np.exp(logprob(predictions, labels))\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Tunining weights on Harry Potter\")\n",
    "    # Tune on Harry Potter\n",
    "    \n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = tune_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict = feed_dict)\n",
    "        \n",
    "        mean_loss += l\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                \n",
    "            print(\"Average loss at step %d: %f learning rate: %f\" % (step, mean_loss, lr))\n",
    "            \n",
    "            mean_loss = 0\n",
    "            \n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print(\"Minibatch perplexity: %.2f\" % float(\n",
    "                np.exp(logprob(predictions, labels))\n",
    "            ))\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Mesaure validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print(\"Validation set perplexity: %.2f\" % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

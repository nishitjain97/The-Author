{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TheAuthor.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fREXEhW7FXH"
      },
      "source": [
        "# The Author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-16i55Zk7FXN"
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGVwNpTZ7FXP",
        "outputId": "6d06722b-47dd-447b-88aa-96471506cf2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified %s' % filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "    return filename\n",
        "\n",
        "filename = maybe_download('./drive/MyDrive/Dataset/text8.zip', 31344016)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified ./drive/MyDrive/Dataset/text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5an1aoD97FXQ",
        "outputId": "ecd8cd48-a823-4d22-a29e-fc8313d08431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def clean_character(char):\n",
        "    if char in string.ascii_lowercase:\n",
        "        return char\n",
        "    elif char in string.ascii_uppercase:\n",
        "        return char.lower()\n",
        "    elif char == ' ':\n",
        "        return char\n",
        "    return ''\n",
        "\n",
        "def read_data(filename):\n",
        "    if os.path.splitext(filename)[1] == '.zip':\n",
        "        with zipfile.ZipFile(filename) as f:\n",
        "            name = f.namelist()[0]\n",
        "            data = tf.compat.as_str(f.read(name))\n",
        "    elif os.path.splitext(filename)[1] == '.txt':\n",
        "        with open(filename, 'r') as f:\n",
        "            data = f.readlines()\n",
        "            data = ''.join(data)\n",
        "    return data\n",
        "  \n",
        "text8 = read_data(filename)\n",
        "text8 = ''.join([clean_character(char) for char in text8])\n",
        "\n",
        "hp_path = \"./drive/MyDrive/Dataset/HarryPotter/\"\n",
        "harry_potter = ''.join([read_data(filename) \n",
        "                        for filename in [os.path.join(hp_path, path) \n",
        "                                         for path in os.listdir(hp_path)]])\n",
        "harry_potter = ''.join([clean_character(char) for char in harry_potter])\n",
        "print('Data size Text8 %d' % len(text8))\n",
        "print('Data size Harry Potter %d' % len(harry_potter))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size Text8 100000000\n",
            "Data size Harry Potter 5893451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGM3M2nN7FXR",
        "outputId": "7f85a308-a5c1-41a1-96eb-8307f9112e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
        "first_letter = ord(string.ascii_lowercase[0])\n",
        "\n",
        "def char2id(char):\n",
        "    if char in string.ascii_lowercase:\n",
        "        return ord(char) - first_letter + 1\n",
        "    elif char == ' ':\n",
        "        return 0\n",
        "    else:\n",
        "        print('Unexpected character: %s' % char)\n",
        "        return 0\n",
        "    \n",
        "def id2char(dictid):\n",
        "    if dictid > 0:\n",
        "        return chr(dictid + first_letter - 1)\n",
        "    else:\n",
        "        return ' '\n",
        "\n",
        "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
        "print(id2char(1), id2char(26), id2char(0))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unexpected character: ï\n",
            "1 26 0 0\n",
            "a z  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be9gWY647FXR",
        "outputId": "de359067-d4ef-492f-ff08-1ab8699f3d25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "valid_size = 1000\n",
        "train_text = text8\n",
        "tune_text = harry_potter[:valid_size]\n",
        "valid_text = harry_potter[valid_size:]\n",
        "train_size = len(train_text)\n",
        "tune_size = len(tune_text)\n",
        "print(train_size + tune_size, train_text[:64])\n",
        "print(valid_size, valid_text[:64])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100001000  anarchism originated as a term of abuse first used against earl\n",
            "1000 nd the sorcerers stone  by jk rowlingp cmsummary rescued from th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBba07vu7FXS",
        "outputId": "813125ca-03a6-4dee-e641-ca14b0c5bd81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 64\n",
        "num_unrollings = 10\n",
        "\n",
        "class BatchGenerator(object):\n",
        "    def __init__(self, text, batch_size, num_unrollings):\n",
        "        self._text = text\n",
        "        self._text_size = len(text)\n",
        "        self._batch_size = batch_size\n",
        "        self._num_unrollings = num_unrollings\n",
        "        segment = self._text_size // batch_size\n",
        "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
        "        self._last_batch = self._next_batch()\n",
        "        \n",
        "    def _next_batch(self):\n",
        "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
        "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
        "        for b in range(self._batch_size):\n",
        "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
        "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
        "        return batch\n",
        "    \n",
        "    def next(self):\n",
        "        \"\"\"\n",
        "            Generate the next array of batches from the data. The array consists of\n",
        "            the last batch of the previous array, followed by num_unrollings new ones.\n",
        "        \"\"\"\n",
        "        batches = [self._last_batch]\n",
        "        for step in range(self._num_unrollings):\n",
        "            batches.append(self._next_batch())\n",
        "        self._last_batch = batches[-1]\n",
        "        return batches\n",
        "    \n",
        "def characters(probabilities):\n",
        "    \"\"\"\n",
        "        Turn a 1-hot encoding or a probability distribution over the possible characters\n",
        "        back into its (most likely) character representation.\n",
        "    \"\"\"\n",
        "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
        "\n",
        "def batches2string(batches):\n",
        "    \"\"\"\n",
        "        Convert a sequence of batches back into their (most likely) string representation\n",
        "    \"\"\"\n",
        "    s = [''] * batches[0].shape[0]\n",
        "    for b in batches:\n",
        "        s = [''.join(x) for x in zip(s, characters(b))]\n",
        "    return s\n",
        "\n",
        "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
        "tune_batches = BatchGenerator(tune_text, batch_size, num_unrollings)\n",
        "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
        "\n",
        "print(batches2string(train_batches.next()))\n",
        "print(batches2string(train_batches.next()))\n",
        "print(batches2string(tune_batches.next()))\n",
        "print(batches2string(tune_batches.next()))\n",
        "print(batches2string(valid_batches.next()))\n",
        "print(batches2string(valid_batches.next()))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' anarchism ', 'ion from eu', 'esident lyn', 'r professio', ' minting af', 'concentrati', 'poken today', 'ber of hote', ' the bell a', 'overty with', 'anufacturer', ' britannica', 'd so david ', 'ea of worke', ' informatio', ' one hand a', 'hannel isla', 'ater reprin', 'r an invest', ' infinite a', 's explained', 'on of plant', ' be solved ', 'n nine resp', 'ation vital', 'self contai', 'ountless se', 'ce arag leg', 'are often c', 'ions eml th', 'ne such as ', 's livin thi', 'ecommunicat', ' one nine n', 'e six two z', ' computer g', 'ing to quan', 'erbicide to', ' special re', ' exotic per', 'rm indicati', 'a secular r', 'the frequen', ' synthesize', 'structural ', ' the media ', 'ail of tear', 'stems able ', 'anded by si', 'rophy assoc', ' union s pi', 'ired differ', 'hough not i', 'e minister ', 'y the king ', 'eudo histor', 'nounced at ', 'tics gullbe', 'sance compa', ' colleges w', 'et and sunr', 'e american ', 'he mass app', 'made such d']\n",
            "[' originated', 'urope aided', 'ndon johnso', 'on allowanc', 'fonso also ', 'ion camp wh', 'y as a firs', 'els and fin', 'aircraft fa', 'h the help ', 'rs such as ', 'a successio', ' collects t', 'ers self ma', 'on the meda', 'and removin', 'and french ', 'nted in boo', 'tigation of', 'amount of t', 'd above is ', 't shoots cr', ' and which ', 'pectively a', 'l for contr', 'ined system', 'exual perve', 'gal insuran', 'called in b', 'his is used', ' esoteric b', 'ing remade ', 'tions techn', 'nine two th', 'zero zero f', 'game afterb', 'ntum mechan', 'o the exten', 'elativity s', 'rfections t', 'ing that th', 'republic la', 'ncy r sqrt ', 'ed retrovir', ' reforms fa', ' announced ', 'rs the rise', ' to run on ', 'ir harry sm', 'ciation a c', 'iatiletka o', 'rent combin', 'itself guil', ' final year', ' serves a l', 'ry geoffrey', ' ricky s cl', 'erg jan mat', 'any and a m', 'which gradu', 'rise occur ', ' indian res', 'peal of dev', 'devices pos']\n",
            "['harry potte', 'd the sorce', ' stonebyj k', 'lingillustr', 'ns by mary ', 'dprarthur a', 'ine booksan', 'rint of sch', 'tic pressfo', 'ssica who l', ' storiesfor', 'e who loved', 'm tooand fo', ' who heard ', ' one firstt', 'copyright  ', 'jk rowlingi', 'trations by', 'y grandpr c', 'ight   warn', 'rosall righ', 'eserved pub', 'ed by schol', 'c press a d', 'ion of scho', 'ic incpubli', 's since sch', 'tic scholas', 'press and t', 'antern logo', 'trademarks ', 'r registere', 'ademarks of', 'olastic inc', 'y potter an', 'l related c', 'cters and e', 'nts are tra', 'rks of warn', 'rosno part ', 'his publica', ' may be rep', 'ced or stor', 'n a retriev', 'ystem or tr', 'itted in an', 'rm or by an', 'ans electro', 'mechanical ', 'ocopying re', 'ing or othe', 'e without w', 'en permissi', 'f the publi', ' for inform', 'n regarding', 'missions wr', 'to scholast', 'nc attentio', 'rmissions d', 'tment  broa', ' new york n', 'brary of co', 'ss catalogi']\n",
            "['er and the ', 'erers stone', 'k rowlingil', 'rations by ', ' grandprart', 'a levine bo', 'n imprint o', 'holastic pr', 'or jessica ', 'loves stori', 'r anne who ', 'd them tooa', 'or di who h', ' this one f', 'text copyri', '  by jk row', 'illustratio', 'y mary gran', 'copyright  ', 'ner brosall', 'hts reserve', 'blished by ', 'lastic pres', 'division of', 'olastic inc', 'ishers sinc', 'holastic sc', 'stic press ', 'the lantern', 'oare tradem', ' andor regi', 'ed trademar', 'f scholasti', 'charry pott', 'nd all rela', 'characters ', 'elements ar', 'ademarks of', 'ner brosno ', ' of this pu', 'ation may b', 'produced or', 'red in a re', 'val system ', 'ransmitted ', 'ny form or ', 'ny means el', 'onic mechan', ' photocopyi', 'ecording or', 'erwise with', 'written per', 'ion of the ', 'isher for i', 'mation rega', 'g permissio', 'rite to sch', 'tic inc att', 'on permissi', 'department ', 'adway new y', 'ny library ', 'ongress cat', 'inginpublic']\n",
            "['nd']\n",
            "['d ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8D13l4n7FXT"
      },
      "source": [
        "def logprob(predictions, labels):\n",
        "    \"\"\"\n",
        "        Log-probability of the true labels in a predicted batch.\n",
        "    \"\"\"\n",
        "    predictions[predictions < 1e-10] = 1e-10\n",
        "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
        "\n",
        "def sample_distribution(distribution):\n",
        "    \"\"\"\n",
        "        Sample one element from a distribution assumed to be an array of normalized\n",
        "        probabilities.\n",
        "    \"\"\"\n",
        "    r = random.uniform(0, 1)\n",
        "    s = 0\n",
        "    for i in range(len(distribution)):\n",
        "        s += distribution[i]\n",
        "        if s >= r:\n",
        "            return i\n",
        "    return len(distribution) - 1\n",
        "\n",
        "def sample(prediction):\n",
        "    \"\"\"\n",
        "        Turn a (column) prediction into 1-hot encoded samples\n",
        "    \"\"\"\n",
        "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
        "    p[0, sample_distribution(prediction[0])] = 1.0\n",
        "    return p\n",
        "\n",
        "def random_distribution():\n",
        "    \"\"\"\n",
        "        Generate a random column of probabilities\n",
        "    \"\"\"\n",
        "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
        "    return b / np.sum(b, 1)[:, None]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoJyMoWK7FXU"
      },
      "source": [
        "num_nodes = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    # Parameters:\n",
        "    # Input gate: input, previous output, bias\n",
        "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Forget gate: input, previous output, bias\n",
        "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Memory cell: input, previous output, bias\n",
        "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Output gate: input, previous output, bias\n",
        "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # Variables saving state across unrollings.\n",
        "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "    \n",
        "    # Classifier weights and biases\n",
        "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
        "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "    \n",
        "    # Define cell computation\n",
        "    def lstm_cell(i, o, state):\n",
        "        \"\"\"\n",
        "            Create LSTM cell.\n",
        "        \"\"\"\n",
        "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
        "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
        "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
        "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
        "        return output_gate * tf.tanh(state), state\n",
        "    \n",
        "    # Input data\n",
        "    train_data = list()\n",
        "    for _ in range(num_unrollings + 1):\n",
        "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
        "    train_inputs = train_data[:num_unrollings]\n",
        "    train_labels = train_data[1:]\n",
        "    \n",
        "    # Unrolled LSTM loop\n",
        "    outputs = list()\n",
        "    output = saved_output\n",
        "    state = saved_state\n",
        "    for i in train_inputs:\n",
        "        output, state = lstm_cell(i, output, state)\n",
        "        outputs.append(output)\n",
        "        \n",
        "    # State saving across unrollings\n",
        "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
        "        # Classifier\n",
        "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = tf.concat(train_labels, 0),\n",
        "                                                                      logits = logits))\n",
        "    \n",
        "    # Optimizer\n",
        "    global_step = tf.Variable(0)\n",
        "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
        "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
        "    \n",
        "    # Predictions\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    \n",
        "    # Sampling and validation eval: batch 1, no unrolling\n",
        "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
        "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    reset_sample_state = tf.group(\n",
        "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
        "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
        "    )\n",
        "    sample_output, sample_state = lstm_cell(sample_input, \n",
        "                                            saved_sample_output, \n",
        "                                            saved_sample_state)\n",
        "    \n",
        "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
        "                                  saved_sample_state.assign(sample_state)]):\n",
        "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXc2oiM17FXW",
        "outputId": "ea8006eb-94c9-45fa-cc31-7253912c939a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_steps = 10000\n",
        "summary_frequency = 100\n",
        "\n",
        "with tf.Session(graph = graph) as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print(\"Initialized\")\n",
        "    \n",
        "    print(\"Training weights on Text8\")\n",
        "    # Train on Text8\n",
        "    mean_loss = 0\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        batches = train_batches.next()\n",
        "        feed_dict = dict()\n",
        "        \n",
        "        for i in range(num_unrollings + 1):\n",
        "            feed_dict[train_data[i]] = batches[i]\n",
        "            \n",
        "        _, l, predictions, lr = session.run(\n",
        "            [optimizer, loss, train_prediction, learning_rate], feed_dict = feed_dict)\n",
        "        \n",
        "        mean_loss += l\n",
        "        \n",
        "        if step % summary_frequency == 0:\n",
        "            if step > 0:\n",
        "                mean_loss = mean_loss / summary_frequency\n",
        "                \n",
        "            print(\"Average loss at step %d: %f learning rate: %f\" % (step, mean_loss, lr))\n",
        "            \n",
        "            mean_loss = 0\n",
        "            \n",
        "            labels = np.concatenate(list(batches)[1:])\n",
        "            print(\"Minibatch perplexity: %.2f\" % float(\n",
        "                np.exp(logprob(predictions, labels))\n",
        "            ))\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(\"Tunining weights on Harry Potter\")\n",
        "    # Tune on Harry Potter\n",
        "    \n",
        "    mean_loss = 0\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        batches = tune_batches.next()\n",
        "        feed_dict = dict()\n",
        "        \n",
        "        for i in range(num_unrollings + 1):\n",
        "            feed_dict[train_data[i]] = batches[i]\n",
        "            \n",
        "        _, l, predictions, lr = session.run(\n",
        "            [optimizer, loss, train_prediction, learning_rate], feed_dict = feed_dict)\n",
        "        \n",
        "        mean_loss += l\n",
        "        \n",
        "        if step % summary_frequency == 0:\n",
        "            if step > 0:\n",
        "                mean_loss = mean_loss / summary_frequency\n",
        "                \n",
        "            print(\"Average loss at step %d: %f learning rate: %f\" % (step, mean_loss, lr))\n",
        "            \n",
        "            mean_loss = 0\n",
        "            \n",
        "            labels = np.concatenate(list(batches)[1:])\n",
        "            print(\"Minibatch perplexity: %.2f\" % float(\n",
        "                np.exp(logprob(predictions, labels))\n",
        "            ))\n",
        "            \n",
        "            if step % (summary_frequency * 10) == 0:\n",
        "                # Generate some samples\n",
        "                print('=' * 80)\n",
        "                for _ in range(5):\n",
        "                    feed = sample(random_distribution())\n",
        "                    sentence = characters(feed)[0]\n",
        "                    reset_sample_state.run()\n",
        "                    for _ in range(79):\n",
        "                        prediction = sample_prediction.eval({sample_input: feed})\n",
        "                        feed = sample(prediction)\n",
        "                        sentence += characters(feed)[0]\n",
        "                    print(sentence)\n",
        "                print('=' * 80)\n",
        "            # Mesaure validation set perplexity\n",
        "            reset_sample_state.run()\n",
        "            valid_logprob = 0\n",
        "            for _ in range(valid_size):\n",
        "                b = valid_batches.next()\n",
        "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
        "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
        "            print(\"Validation set perplexity: %.2f\" % float(np.exp(valid_logprob / valid_size)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Training weights on Text8\n",
            "Average loss at step 0: 3.296016 learning rate: 10.000000\n",
            "Minibatch perplexity: 27.00\n",
            "Average loss at step 100: 2.600191 learning rate: 10.000000\n",
            "Minibatch perplexity: 11.49\n",
            "Average loss at step 200: 2.273329 learning rate: 10.000000\n",
            "Minibatch perplexity: 9.14\n",
            "Average loss at step 300: 2.121965 learning rate: 10.000000\n",
            "Minibatch perplexity: 8.54\n",
            "Average loss at step 400: 2.011467 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.82\n",
            "Average loss at step 500: 1.947707 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.97\n",
            "Average loss at step 600: 1.940168 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.34\n",
            "Average loss at step 700: 1.892151 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.04\n",
            "Average loss at step 800: 1.845585 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.37\n",
            "Average loss at step 900: 1.817763 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.02\n",
            "Average loss at step 1000: 1.809320 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.82\n",
            "Average loss at step 1100: 1.779416 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.27\n",
            "Average loss at step 1200: 1.781412 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.03\n",
            "Average loss at step 1300: 1.751492 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.27\n",
            "Average loss at step 1400: 1.736552 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.17\n",
            "Average loss at step 1500: 1.709890 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.73\n",
            "Average loss at step 1600: 1.708889 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.80\n",
            "Average loss at step 1700: 1.685421 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.74\n",
            "Average loss at step 1800: 1.658559 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.22\n",
            "Average loss at step 1900: 1.668158 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.40\n",
            "Average loss at step 2000: 1.688003 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.80\n",
            "Average loss at step 2100: 1.690245 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.71\n",
            "Average loss at step 2200: 1.692749 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.96\n",
            "Average loss at step 2300: 1.678375 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.71\n",
            "Average loss at step 2400: 1.667329 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.47\n",
            "Average loss at step 2500: 1.680882 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 2600: 1.647531 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.16\n",
            "Average loss at step 2700: 1.660342 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.15\n",
            "Average loss at step 2800: 1.640637 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.43\n",
            "Average loss at step 2900: 1.658255 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.70\n",
            "Average loss at step 3000: 1.653226 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.57\n",
            "Average loss at step 3100: 1.610314 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.03\n",
            "Average loss at step 3200: 1.623551 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.98\n",
            "Average loss at step 3300: 1.644811 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.29\n",
            "Average loss at step 3400: 1.644663 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.88\n",
            "Average loss at step 3500: 1.666752 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.48\n",
            "Average loss at step 3600: 1.627157 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.42\n",
            "Average loss at step 3700: 1.625004 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 3800: 1.621151 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.01\n",
            "Average loss at step 3900: 1.639432 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.83\n",
            "Average loss at step 4000: 1.623623 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.04\n",
            "Average loss at step 4100: 1.615690 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.04\n",
            "Average loss at step 4200: 1.610523 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.69\n",
            "Average loss at step 4300: 1.593284 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.94\n",
            "Average loss at step 4400: 1.599951 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.93\n",
            "Average loss at step 4500: 1.598360 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.45\n",
            "Average loss at step 4600: 1.608360 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.67\n",
            "Average loss at step 4700: 1.625556 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.12\n",
            "Average loss at step 4800: 1.624170 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.32\n",
            "Average loss at step 4900: 1.614793 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.16\n",
            "Average loss at step 5000: 1.620185 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.87\n",
            "Average loss at step 5100: 1.596928 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.61\n",
            "Average loss at step 5200: 1.626220 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.79\n",
            "Average loss at step 5300: 1.603777 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.54\n",
            "Average loss at step 5400: 1.599972 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.56\n",
            "Average loss at step 5500: 1.620037 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.01\n",
            "Average loss at step 5600: 1.577481 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.24\n",
            "Average loss at step 5700: 1.578449 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.39\n",
            "Average loss at step 5800: 1.588277 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.96\n",
            "Average loss at step 5900: 1.593125 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.19\n",
            "Average loss at step 6000: 1.613977 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.12\n",
            "Average loss at step 6100: 1.592576 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.39\n",
            "Average loss at step 6200: 1.616622 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.85\n",
            "Average loss at step 6300: 1.609967 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.86\n",
            "Average loss at step 6400: 1.604286 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.00\n",
            "Average loss at step 6500: 1.594322 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.72\n",
            "Average loss at step 6600: 1.574157 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.66\n",
            "Average loss at step 6700: 1.573086 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.57\n",
            "Average loss at step 6800: 1.582946 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.42\n",
            "Average loss at step 6900: 1.593766 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.29\n",
            "Average loss at step 7000: 1.590027 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.57\n",
            "Average loss at step 7100: 1.595905 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.04\n",
            "Average loss at step 7200: 1.590972 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.02\n",
            "Average loss at step 7300: 1.586144 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.80\n",
            "Average loss at step 7400: 1.583482 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.26\n",
            "Average loss at step 7500: 1.575591 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.56\n",
            "Average loss at step 7600: 1.587466 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.98\n",
            "Average loss at step 7700: 1.577811 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.56\n",
            "Average loss at step 7800: 1.565354 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.69\n",
            "Average loss at step 7900: 1.599381 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.09\n",
            "Average loss at step 8000: 1.566140 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.94\n",
            "Average loss at step 8100: 1.556071 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.79\n",
            "Average loss at step 8200: 1.575336 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.18\n",
            "Average loss at step 8300: 1.569036 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.65\n",
            "Average loss at step 8400: 1.575351 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.74\n",
            "Average loss at step 8500: 1.574085 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.56\n",
            "Average loss at step 8600: 1.567339 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.15\n",
            "Average loss at step 8700: 1.580380 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.08\n",
            "Average loss at step 8800: 1.564116 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.58\n",
            "Average loss at step 8900: 1.540297 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.03\n",
            "Average loss at step 9000: 1.568850 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.50\n",
            "Average loss at step 9100: 1.580264 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.72\n",
            "Average loss at step 9200: 1.550538 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.54\n",
            "Average loss at step 9300: 1.569315 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.65\n",
            "Average loss at step 9400: 1.553272 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.72\n",
            "Average loss at step 9500: 1.590240 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.85\n",
            "Average loss at step 9600: 1.588576 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.77\n",
            "Average loss at step 9700: 1.553631 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.18\n",
            "Average loss at step 9800: 1.535505 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.88\n",
            "Average loss at step 9900: 1.575480 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.68\n",
            "Tunining weights on Harry Potter\n",
            "Average loss at step 0: 2.206742 learning rate: 0.100000\n",
            "Minibatch perplexity: 9.09\n",
            "================================================================================\n",
            "lang s agrene planuza sulthatians scliency mult victlen in one nine nine nine as\n",
            "zer the quitods these the reference the incontingtion be constantagia rates este\n",
            "ch one two one zero nine sm agring in anch in the gene armilisch a rign what als\n",
            "der or has people with marst artiful of the with the posiances to lare with it s\n",
            "ven suncen as and world to the lunguastory their each aundu matabilm four one ni\n",
            "================================================================================\n",
            "Validation set perplexity: 7.21\n",
            "Average loss at step 100: 1.741425 learning rate: 0.100000\n",
            "Minibatch perplexity: 5.07\n",
            "Validation set perplexity: 6.23\n",
            "Average loss at step 200: 1.562429 learning rate: 0.100000\n",
            "Minibatch perplexity: 4.40\n",
            "Validation set perplexity: 7.02\n",
            "Average loss at step 300: 1.443188 learning rate: 0.100000\n",
            "Minibatch perplexity: 3.96\n",
            "Validation set perplexity: 6.56\n",
            "Average loss at step 400: 1.350218 learning rate: 0.100000\n",
            "Minibatch perplexity: 3.64\n",
            "Validation set perplexity: 8.41\n",
            "Average loss at step 500: 1.272389 learning rate: 0.100000\n",
            "Minibatch perplexity: 3.38\n",
            "Validation set perplexity: 8.04\n",
            "Average loss at step 600: 1.206036 learning rate: 0.100000\n",
            "Minibatch perplexity: 3.18\n",
            "Validation set perplexity: 9.09\n",
            "Average loss at step 700: 1.147993 learning rate: 0.100000\n",
            "Minibatch perplexity: 3.00\n",
            "Validation set perplexity: 8.11\n",
            "Average loss at step 800: 1.095706 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.86\n",
            "Validation set perplexity: 9.12\n",
            "Average loss at step 900: 1.048639 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.73\n",
            "Validation set perplexity: 9.28\n",
            "Average loss at step 1000: 1.006488 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.63\n",
            "================================================================================\n",
            "gans of vilustory carchoon army two six one nine six six nine finstant likely ri\n",
            "jocation scholazocoping producting repormatary inclpbirs in a philoiogn zero esp\n",
            "ing military or browning wh browningy and purterreds for the largers and reppind\n",
            "jokars pows sceep and yerrowsturostic of a potter of them granks roogra attentor\n",
            "quipted or critation stored by a percharry regartium ppoter or gransemartrand po\n",
            "================================================================================\n",
            "Validation set perplexity: 9.75\n",
            "Average loss at step 1100: 0.967988 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.53\n",
            "Validation set perplexity: 9.34\n",
            "Average loss at step 1200: 0.932383 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.45\n",
            "Validation set perplexity: 9.17\n",
            "Average loss at step 1300: 0.899128 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.37\n",
            "Validation set perplexity: 9.44\n",
            "Average loss at step 1400: 0.867890 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.30\n",
            "Validation set perplexity: 10.21\n",
            "Average loss at step 1500: 0.838342 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.24\n",
            "Validation set perplexity: 11.53\n",
            "Average loss at step 1600: 0.810213 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.18\n",
            "Validation set perplexity: 11.58\n",
            "Average loss at step 1700: 0.783331 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.13\n",
            "Validation set perplexity: 10.67\n",
            "Average loss at step 1800: 0.757518 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.07\n",
            "Validation set perplexity: 10.82\n",
            "Average loss at step 1900: 0.732919 learning rate: 0.100000\n",
            "Minibatch perplexity: 2.03\n",
            "Validation set perplexity: 11.58\n",
            "Average loss at step 2000: 0.709576 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.98\n",
            "================================================================================\n",
            "publication on the lust trademary writtening calogy and the list press traped in\n",
            "jotor and screceresor or storces sort labig partypor rattliye and the publishar \n",
            "ert photoscer permissions them schollocations kdowlingure and it wr thoughespori\n",
            "tally copyrighted for stored by k one stonoming to stored or decastyporld tradem\n",
            "m lobolage new on the loved by arm rightarly inpurrisation over a burgotarrians \n",
            "================================================================================\n",
            "Validation set perplexity: 11.49\n",
            "Average loss at step 2100: 0.686903 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.94\n",
            "Validation set perplexity: 11.58\n",
            "Average loss at step 2200: 0.664930 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.90\n",
            "Validation set perplexity: 12.05\n",
            "Average loss at step 2300: 0.643652 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.86\n",
            "Validation set perplexity: 13.89\n",
            "Average loss at step 2400: 0.623056 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.83\n",
            "Validation set perplexity: 17.24\n",
            "Average loss at step 2500: 0.602991 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.79\n",
            "Validation set perplexity: 15.14\n",
            "Average loss at step 2600: 0.583568 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.76\n",
            "Validation set perplexity: 13.31\n",
            "Average loss at step 2700: 0.564788 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.73\n",
            "Validation set perplexity: 18.40\n",
            "Average loss at step 2800: 0.546362 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.70\n",
            "Validation set perplexity: 16.76\n",
            "Average loss at step 2900: 0.528259 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.67\n",
            "Validation set perplexity: 19.72\n",
            "Average loss at step 3000: 0.510735 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.65\n",
            "================================================================================\n",
            "grandpr by mary k de dift i potter by storees net an encopart rerrandint and thi\n",
            "quinglated by an them side andered ard storiestation perpersiopal press by the l\n",
            " permersor pressotomy s pressfor y y for lighol livaring linylow scholasticly an\n",
            "grantfrent brosed low stony by are qurrf attene coperements bro ninee smanynk by\n",
            "tation warner that press and the last by s storiessa percated by reqeadrark phot\n",
            "================================================================================\n",
            "Validation set perplexity: 19.48\n",
            "Average loss at step 3100: 0.493690 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.62\n",
            "Validation set perplexity: 21.46\n",
            "Average loss at step 3200: 0.477097 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.59\n",
            "Validation set perplexity: 13.87\n",
            "Average loss at step 3300: 0.460974 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.57\n",
            "Validation set perplexity: 19.96\n",
            "Average loss at step 3400: 0.445336 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.55\n",
            "Validation set perplexity: 20.80\n",
            "Average loss at step 3500: 0.430207 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.53\n",
            "Validation set perplexity: 20.25\n",
            "Average loss at step 3600: 0.415723 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.51\n",
            "Validation set perplexity: 18.09\n",
            "Average loss at step 3700: 0.401870 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.49\n",
            "Validation set perplexity: 17.93\n",
            "Average loss at step 3800: 0.388335 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.47\n",
            "Validation set perplexity: 19.74\n",
            "Average loss at step 3900: 0.374925 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.45\n",
            "Validation set perplexity: 20.43\n",
            "Average loss at step 4000: 0.361822 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.43\n",
            "================================================================================\n",
            "bootorivatic preved broadwry by tred trademary of coland storiesfor k press and \n",
            "ard any scholastintect sto by are permis of congregt of scholastic press are any\n",
            "form of thistaro brosbodogarrive ppotatived in america lovoviciement by grandpro\n",
            "forms who loves and this collss andeen trademarks of scholastic jury scholasters\n",
            "harrung inckelism governm it nargly copyright without and or transmic recording \n",
            "================================================================================\n",
            "Validation set perplexity: 20.74\n",
            "Average loss at step 4100: 0.349286 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.42\n",
            "Validation set perplexity: 30.03\n",
            "Average loss at step 4200: 0.337532 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.40\n",
            "Validation set perplexity: 27.33\n",
            "Average loss at step 4300: 0.326523 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.38\n",
            "Validation set perplexity: 29.07\n",
            "Average loss at step 4400: 0.315969 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.37\n",
            "Validation set perplexity: 27.68\n",
            "Average loss at step 4500: 0.310643 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.36\n",
            "Validation set perplexity: 27.72\n",
            "Average loss at step 4600: 0.300756 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.35\n",
            "Validation set perplexity: 24.57\n",
            "Average loss at step 4700: 0.290680 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.34\n",
            "Validation set perplexity: 29.84\n",
            "Average loss at step 4800: 0.282824 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.33\n",
            "Validation set perplexity: 23.29\n",
            "Average loss at step 4900: 0.276962 learning rate: 0.100000\n",
            "Minibatch perplexity: 1.32\n",
            "Validation set perplexity: 22.50\n",
            "Average loss at step 5000: 0.271246 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.31\n",
            "================================================================================\n",
            "phelator postocien partrady recordintered for by le forky deparssicuuptraticapau\n",
            "landern ristriciuterch brosall recording or othark pressfor intereced inchark in\n",
            "o barvingly whoberem relastern ulstyco trademarks by stre tradema distends of co\n",
            " wristfre shobtlicar pert rivisutions congressens any these without written pota\n",
            "blse teeval corcye charrgirg ropores and e former regardistinceware written pero\n",
            "================================================================================\n",
            "Validation set perplexity: 25.26\n",
            "Average loss at step 5100: 0.266776 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 23.96\n",
            "Average loss at step 5200: 0.263023 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.31\n",
            "Validation set perplexity: 31.04\n",
            "Average loss at step 5300: 0.261734 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 28.34\n",
            "Average loss at step 5400: 0.259262 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.31\n",
            "Validation set perplexity: 26.58\n",
            "Average loss at step 5500: 0.261698 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 29.40\n",
            "Average loss at step 5600: 0.275689 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.33\n",
            "Validation set perplexity: 23.56\n",
            "Average loss at step 5700: 0.272983 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.31\n",
            "Validation set perplexity: 23.53\n",
            "Average loss at step 5800: 0.269025 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 28.80\n",
            "Average loss at step 5900: 0.255952 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 21.90\n",
            "Average loss at step 6000: 0.254175 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "================================================================================\n",
            "danckers who hoo pottly permi pharacting straitmative case trademarks storiesfor\n",
            "form trossion ocarrger port nitorenticia peltralktto bringorary scholastic press\n",
            "x maryly borkswelfarreinchuraters are browning a reproduscare potter and eight t\n",
            "mary and terephers of scholastic press a more by mary boo anyi zerrowresfr this \n",
            "n retribasisce in anne writest trodenced and ecobol devilusoncrighed or gingze o\n",
            "================================================================================\n",
            "Validation set perplexity: 31.00\n",
            "Average loss at step 6100: 0.254191 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 27.02\n",
            "Average loss at step 6200: 0.251981 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 23.87\n",
            "Average loss at step 6300: 0.257811 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 30.69\n",
            "Average loss at step 6400: 0.258481 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.30\n",
            "Validation set perplexity: 25.89\n",
            "Average loss at step 6500: 0.254785 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 30.68\n",
            "Average loss at step 6600: 0.249922 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 36.73\n",
            "Average loss at step 6700: 0.245662 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 23.56\n",
            "Average loss at step 6800: 0.246501 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 29.72\n",
            "Average loss at step 6900: 0.251128 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 26.46\n",
            "Average loss at step 7000: 0.247449 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "================================================================================\n",
            " how j by by mary belietary booksan incurstalitication of congress further a har\n",
            "charry photociementication of tranmemoths form tooppeardry by mbrotative mark gr\n",
            "formm who hanarl grandmare who loves chase a repropomare published potting of th\n",
            "ull mary copyrigho levtropregor justrond and or  this scholastoceded them tailos\n",
            "viliticions de presed one formithind for y rowling reprohtck for incharners are \n",
            "================================================================================\n",
            "Validation set perplexity: 27.01\n",
            "Average loss at step 7100: 0.245246 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 31.32\n",
            "Average loss at step 7200: 0.245613 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 28.28\n",
            "Average loss at step 7300: 0.245369 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 25.98\n",
            "Average loss at step 7400: 0.245613 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 25.54\n",
            "Average loss at step 7500: 0.241795 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 26.35\n",
            "Average loss at step 7600: 0.241312 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 25.93\n",
            "Average loss at step 7700: 0.241739 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 22.08\n",
            "Average loss at step 7800: 0.241495 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 23.80\n",
            "Average loss at step 7900: 0.237629 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 26.89\n",
            "Average loss at step 8000: 0.236617 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "================================================================================\n",
            "quantvousling in one fororanck part of the potter ardical permis armarow bod mar\n",
            " for gipre permissions toiation one formic microstagelincements by anytrepe or s\n",
            "ch storogy a loves storher grandprautren who howlandary this otherwisting by jan\n",
            "lanture this publication pressfor jarae thoses arewitating of the lovosticlybans\n",
            "k properart or chranisap with jraws mystraters library or scholastic incpublicat\n",
            "================================================================================\n",
            "Validation set perplexity: 26.59\n",
            "Average loss at step 8100: 0.237814 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 20.17\n",
            "Average loss at step 8200: 0.235700 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 23.90\n",
            "Average loss at step 8300: 0.234123 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 24.86\n",
            "Average loss at step 8400: 0.233220 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 28.09\n",
            "Average loss at step 8500: 0.233968 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 20.18\n",
            "Average loss at step 8600: 0.235415 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 30.50\n",
            "Average loss at step 8700: 0.235965 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 33.79\n",
            "Average loss at step 8800: 0.243547 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 35.38\n",
            "Average loss at step 8900: 0.234725 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 29.57\n",
            "Average loss at step 9000: 0.231374 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "================================================================================\n",
            "r on the publimillshard who loves storiemincy andor regint othon how requers sto\n",
            "d trademarks known rk sky electronmentary vietract potter and them tooage skeb b\n",
            "k a rittarow pottery notoresmar presed or is a relautered kk des droadedy who lo\n",
            "ch s d for jegersbarict broand potting regemented retried arder broaks written p\n",
            "quing stokensure who hapsentar stofor library k relai tradema potter and e roruc\n",
            "================================================================================\n",
            "Validation set perplexity: 28.27\n",
            "Average loss at step 9100: 0.231273 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 30.55\n",
            "Average loss at step 9200: 0.230430 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 24.53\n",
            "Average loss at step 9300: 0.226097 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 27.01\n",
            "Average loss at step 9400: 0.238530 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.29\n",
            "Validation set perplexity: 24.04\n",
            "Average loss at step 9500: 0.246553 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 27.46\n",
            "Average loss at step 9600: 0.235153 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.27\n",
            "Validation set perplexity: 31.94\n",
            "Average loss at step 9700: 0.234219 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.28\n",
            "Validation set perplexity: 32.00\n",
            "Average loss at step 9800: 0.230565 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 28.96\n",
            "Average loss at step 9900: 0.226376 learning rate: 0.010000\n",
            "Minibatch perplexity: 1.26\n",
            "Validation set perplexity: 31.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3kEIACa7FXX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}